{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Utilz A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Home"},{"location":"#utilz","text":"A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Utilz"},{"location":"api/guards/","text":"Guards and Decorators Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators, e.g. @log_df def myfunc(df): do some stuff... disk_cache ( threshold = 60 , autoload = True , index = False , verbose = False ) Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 60. 60 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool; optional whether to incluce the index when saving a dataframe. Default to False False verbose bool; optional print debug messages; Default to False False Source code in utilz/guards.py def disk_cache ( threshold = 60 , autoload = True , index = False , verbose = False ): \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 60. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False verbose (bool; optional): print debug messages; Default to False \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): filtered_args = [ e for e in args if not isinstance ( e , ( list , pd . DataFrame , np . ndarray )) ] key = str (( filtered_args , tuple ( sorted ( kwargs . items ())))) key_csv = f \" { func . __name__ } _ { key } .csv\" key_h5 = f \" { func . __name__ } _ { key } .h5\" if autoload : if Path ( key_csv ) . exists (): if verbose : print ( \"Returning cached result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): if verbose : print ( \"Returning cached result\" ) return dd . io . load ( key_h5 ) if verbose : print ( \"No cached result...executing\" ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): fname = f \" { func . __name__ } _ { key } .csv\" result . to_csv ( fname , index = index ) else : fname = f \" { func . __name__ } _ { key } .h5\" dd . io . save ( fname , result , compression = \"zlib\" ) print ( f \"Exceeded threshold. Result cached to { fname } \" ) return result return wrapper return decorator log_df ( func ) Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper","title":"Guards and Decorators"},{"location":"api/guards/#guards-and-decorators","text":"","title":"Guards and Decorators"},{"location":"api/guards/#utilz.guards","text":"Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators, e.g. @log_df def myfunc(df): do some stuff...","title":"utilz.guards"},{"location":"api/guards/#utilz.guards.disk_cache","text":"Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 60. 60 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool; optional whether to incluce the index when saving a dataframe. Default to False False verbose bool; optional print debug messages; Default to False False Source code in utilz/guards.py def disk_cache ( threshold = 60 , autoload = True , index = False , verbose = False ): \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 60. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False verbose (bool; optional): print debug messages; Default to False \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): filtered_args = [ e for e in args if not isinstance ( e , ( list , pd . DataFrame , np . ndarray )) ] key = str (( filtered_args , tuple ( sorted ( kwargs . items ())))) key_csv = f \" { func . __name__ } _ { key } .csv\" key_h5 = f \" { func . __name__ } _ { key } .h5\" if autoload : if Path ( key_csv ) . exists (): if verbose : print ( \"Returning cached result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): if verbose : print ( \"Returning cached result\" ) return dd . io . load ( key_h5 ) if verbose : print ( \"No cached result...executing\" ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): fname = f \" { func . __name__ } _ { key } .csv\" result . to_csv ( fname , index = index ) else : fname = f \" { func . __name__ } _ { key } .h5\" dd . io . save ( fname , result , compression = \"zlib\" ) print ( f \"Exceeded threshold. Result cached to { fname } \" ) return result return wrapper return decorator","title":"disk_cache()"},{"location":"api/guards/#utilz.guards.log_df","text":"Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper","title":"log_df()"},{"location":"api/ops/","text":"Operations Common data operations and transformations. Often on pandas dataframes ploop ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx_available = True , backend = 'processes' , progress = True ) Call a function for n_iter using parallelization via joblib Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx_available bool whether the value of the current iteration should be passed as the last argument to func; Default True True backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool, opional whether to show progress; Default True True Source code in utilz/ops.py def ploop ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx_available = True , backend = \"processes\" , progress = True , ): \"\"\" Call a function for n_iter using parallelization via joblib Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx_available (bool, optional): whether the value of the current iteration should be passed as the last argument to func; Default True backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool, opional): whether to show progress; Default True \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) verbose = 10 if progress else 0 parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if func_args is None : if loop_idx_available : out = parfor ( delayed ( func )( i ) for i in range ( n_iter )) else : out = parfor ( delayed ( func ) for i in range ( n_iter )) else : if loop_idx_available : out = parfor ( delayed ( func )( * func_args , i ) for i in range ( n_iter )) else : out = parfor ( delayed ( func )( * func_args ) for i in range ( n_iter )) return out","title":"Operations"},{"location":"api/ops/#operations","text":"","title":"Operations"},{"location":"api/ops/#utilz.ops","text":"Common data operations and transformations. Often on pandas dataframes","title":"utilz.ops"},{"location":"api/ops/#utilz.ops.ploop","text":"Call a function for n_iter using parallelization via joblib Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx_available bool whether the value of the current iteration should be passed as the last argument to func; Default True True backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool, opional whether to show progress; Default True True Source code in utilz/ops.py def ploop ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx_available = True , backend = \"processes\" , progress = True , ): \"\"\" Call a function for n_iter using parallelization via joblib Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx_available (bool, optional): whether the value of the current iteration should be passed as the last argument to func; Default True backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool, opional): whether to show progress; Default True \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) verbose = 10 if progress else 0 parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if func_args is None : if loop_idx_available : out = parfor ( delayed ( func )( i ) for i in range ( n_iter )) else : out = parfor ( delayed ( func ) for i in range ( n_iter )) else : if loop_idx_available : out = parfor ( delayed ( func )( * func_args , i ) for i in range ( n_iter )) else : out = parfor ( delayed ( func )( * func_args ) for i in range ( n_iter )) return out","title":"ploop()"},{"location":"api/pipe/","text":"Pipe utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok. Example usage 1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe() # Some data to work with from seaborn import load_dataset df = load_dataset('iris') 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Pipe Operator"},{"location":"api/pipe/#pipe","text":"utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok.","title":"Pipe"},{"location":"api/pipe/#example-usage","text":"1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe() # Some data to work with from seaborn import load_dataset df = load_dataset('iris') 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Example usage"}]}