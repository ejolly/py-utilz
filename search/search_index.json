{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Utilz A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Home"},{"location":"#utilz","text":"A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Utilz"},{"location":"api/guards/","text":"utilz.guards Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators: from utilz.guards import log_df @log_df def myfunc(df): do some stuff... disk_cache ( threshold = 30 , autoload = True , index = False , save_dir = '.utilz_cache' ) Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to .utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5} Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 30. 30 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool whether to incluce the index when saving a dataframe. Default to False False save_dir str location to cache results; Default '.utilz_cache' '.utilz_cache' Source code in utilz/guards.py def disk_cache ( threshold : int = 30 , autoload : bool = True , index : bool = False , save_dir : str = \".utilz_cache\" , ) -> Any : \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to `.utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5}` Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 30. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False save_dir (str; optional): location to cache results; Default '.utilz_cache' \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): saved_inputs = dict ( sorted ( getcallargs ( func , * args , ** kwargs ) . items ())) cache_dir = Path ( save_dir ) if not cache_dir . exists (): cache_dir . mkdir () inputs = {} for k , v in saved_inputs . items (): if k == \"args\" : new_v = [ _hashobj ( e ) for e in v ] inputs [ k ] = new_v elif k == \"kwargs\" : new_v = { kk : _hashobj ( vv ) for kk , vv in v . items ()} inputs [ k ] = new_v else : inputs [ k ] = _hashobj ( v ) key = ( dumps ( inputs ) . replace ( '\"' , \"\" ) . replace ( \"{\" , \"\" ) . replace ( \"}\" , \"\" ) . replace ( \"[\" , \"\" ) . replace ( \"]\" , \"\" ) . replace ( \":\" , \"__\" ) . replace ( \" \" , \"\" ) . replace ( \",\" , \"--\" ) ) key_csv = f \" { func . __name__ } ___ { key } .csv\" key_csv = cache_dir . joinpath ( key_csv ) key_h5 = f \" { func . __name__ } ___ { key } .h5\" key_h5 = cache_dir . joinpath ( key_h5 ) if autoload : if Path ( key_csv ) . exists (): print ( \"Returning previously saved result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): print ( \"Returning previously saved result\" ) return dd . io . load ( key_h5 ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): result . to_csv ( str ( key_csv ), index = index ) print ( f \"Exceeded compute time. Result saved to { key_csv } \" ) else : dd . io . save ( str ( key_h5 ), result , compression = \"zlib\" ) print ( f \"Exceeded compute time. Result saved to { key_h5 } \" ) return result return wrapper return decorator log ( func ) Log the type and shape/size/len of the output from a function Parameters: Name Type Description Default func callable any pure function (i.e, has no side-effects) required Source code in utilz/guards.py def log ( func ): \"\"\" Log the type and shape/size/len of the output from a function Args: func (callable): any pure function (i.e, has no side-effects) \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper log_df ( func ) Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper maybe ( filepath , force = False ) Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Parameters: Name Type Description Default filepath Union[pathlib.Path, str] filename or path to check existence for required force bool always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. False Source code in utilz/guards.py def maybe ( filepath : Union [ Path , str ], force : bool = False ) -> Any : \"\"\" Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Args: filepath (Path/str): filename or path to check existence for force (bool, optional): always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): fpath = Path ( filepath ) if not force : if fpath . exists (): return load ( fpath ) else : return func ( * args , ** kwargs ) else : return func ( * args , ** kwargs ) return wrapper return decorator same_nunique ( func , val_col , group_col ) Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required val_col str column name to check for unique values in dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_nunique ( func , val_col , group_col ): \"\"\" Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Args: func (callable): a function that operates on a dataframe val_col (str): column name to check for unique values in dataframe group_call (str): column name to group on in dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper same_size ( func , group_col ) Check if each group of group_col has the same dimensions after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_size ( func , group_col ): \"\"\" Check if each group of group_col has the same dimensions after running a function on a dataframe Args: func (callable): a function that operates on a dataframe group_call (str): column name to group on in dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper","title":"Guards and Decorators"},{"location":"api/guards/#utilzguards","text":"","title":"utilz.guards"},{"location":"api/guards/#utilz.guards","text":"Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators: from utilz.guards import log_df @log_df def myfunc(df): do some stuff...","title":"utilz.guards"},{"location":"api/guards/#utilz.guards.disk_cache","text":"Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to .utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5} Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 30. 30 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool whether to incluce the index when saving a dataframe. Default to False False save_dir str location to cache results; Default '.utilz_cache' '.utilz_cache' Source code in utilz/guards.py def disk_cache ( threshold : int = 30 , autoload : bool = True , index : bool = False , save_dir : str = \".utilz_cache\" , ) -> Any : \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to `.utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5}` Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 30. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False save_dir (str; optional): location to cache results; Default '.utilz_cache' \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): saved_inputs = dict ( sorted ( getcallargs ( func , * args , ** kwargs ) . items ())) cache_dir = Path ( save_dir ) if not cache_dir . exists (): cache_dir . mkdir () inputs = {} for k , v in saved_inputs . items (): if k == \"args\" : new_v = [ _hashobj ( e ) for e in v ] inputs [ k ] = new_v elif k == \"kwargs\" : new_v = { kk : _hashobj ( vv ) for kk , vv in v . items ()} inputs [ k ] = new_v else : inputs [ k ] = _hashobj ( v ) key = ( dumps ( inputs ) . replace ( '\"' , \"\" ) . replace ( \"{\" , \"\" ) . replace ( \"}\" , \"\" ) . replace ( \"[\" , \"\" ) . replace ( \"]\" , \"\" ) . replace ( \":\" , \"__\" ) . replace ( \" \" , \"\" ) . replace ( \",\" , \"--\" ) ) key_csv = f \" { func . __name__ } ___ { key } .csv\" key_csv = cache_dir . joinpath ( key_csv ) key_h5 = f \" { func . __name__ } ___ { key } .h5\" key_h5 = cache_dir . joinpath ( key_h5 ) if autoload : if Path ( key_csv ) . exists (): print ( \"Returning previously saved result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): print ( \"Returning previously saved result\" ) return dd . io . load ( key_h5 ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): result . to_csv ( str ( key_csv ), index = index ) print ( f \"Exceeded compute time. Result saved to { key_csv } \" ) else : dd . io . save ( str ( key_h5 ), result , compression = \"zlib\" ) print ( f \"Exceeded compute time. Result saved to { key_h5 } \" ) return result return wrapper return decorator","title":"disk_cache()"},{"location":"api/guards/#utilz.guards.log","text":"Log the type and shape/size/len of the output from a function Parameters: Name Type Description Default func callable any pure function (i.e, has no side-effects) required Source code in utilz/guards.py def log ( func ): \"\"\" Log the type and shape/size/len of the output from a function Args: func (callable): any pure function (i.e, has no side-effects) \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper","title":"log()"},{"location":"api/guards/#utilz.guards.log_df","text":"Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper","title":"log_df()"},{"location":"api/guards/#utilz.guards.maybe","text":"Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Parameters: Name Type Description Default filepath Union[pathlib.Path, str] filename or path to check existence for required force bool always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. False Source code in utilz/guards.py def maybe ( filepath : Union [ Path , str ], force : bool = False ) -> Any : \"\"\" Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Args: filepath (Path/str): filename or path to check existence for force (bool, optional): always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): fpath = Path ( filepath ) if not force : if fpath . exists (): return load ( fpath ) else : return func ( * args , ** kwargs ) else : return func ( * args , ** kwargs ) return wrapper return decorator","title":"maybe()"},{"location":"api/guards/#utilz.guards.same_nunique","text":"Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required val_col str column name to check for unique values in dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_nunique ( func , val_col , group_col ): \"\"\" Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Args: func (callable): a function that operates on a dataframe val_col (str): column name to check for unique values in dataframe group_call (str): column name to group on in dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper","title":"same_nunique()"},{"location":"api/guards/#utilz.guards.same_size","text":"Check if each group of group_col has the same dimensions after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_size ( func , group_col ): \"\"\" Check if each group of group_col has the same dimensions after running a function on a dataframe Args: func (callable): a function that operates on a dataframe group_call (str): column name to group on in dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper","title":"same_size()"},{"location":"api/io/","text":"utilz.io I/O Module for working with Paths load ( f , as_df = False , as_arr = False , as_str = False , h5_key = 'data' , json_str = False , pickle_encoding = 'rb' , verbose = False , ** kwargs ) A handy dandy all-in-one loading function. Simply pass a Path object to a file (or a string) and you'll back a python object. Supported extensions are: .txt, .csv, .json, .p, .pickle, .h5, .hdf5, .gz Parameters: Name Type Description Default f Union[pathlib.Path, str] name or path object to load required as_df bool treat a .hdf5, .h5 file as a Dataframe; Default False False as_arr bool treat a .txt file as a numpy array; False as_str bool open txt as a single string instead of False json_str bool treat a json file as a string (i.e use json.loads False h5_key str the key within the h5 file to load when if using as_df; 'data' pickle_encoding str pickle encoding to use; Default 'rb' 'rb' verbose bool whether to print messages during load. Default False False **kwargs keyword arguments to pd.read_csv, np.loadtxt, dd.io.load, pickle, or open {} Returns: Type Description Any the loaded object Source code in utilz/io.py def load ( f : Union [ Path , str ], as_df : bool = False , as_arr : bool = False , as_str : bool = False , h5_key : str = \"data\" , json_str : bool = False , pickle_encoding : str = \"rb\" , verbose : bool = False , ** kwargs , ) -> Any : \"\"\" A handy dandy all-in-one loading function. Simply pass a Path object to a file (or a string) and you'll back a python object. Supported extensions are: .txt, .csv, .json, .p, .pickle, .h5, .hdf5, .gz Args: f (Path/str): name or path object to load as_df (bool, optional): treat a .hdf5, .h5 file as a Dataframe; Default False as_arr (bool, optional): treat a .txt file as a numpy array; Default False as_str (bool, optional): open txt as a single string instead of splitting on newlines; Default False json_str (bool, optional): treat a json file as a string (i.e use json.loads instead of json.load); Default False h5_key (str, optional): the key within the h5 file to load when if using as_df; Default 'data' pickle_encoding (str, optional): pickle encoding to use; Default 'rb' verbose (bool, optional): whether to print messages during load. Default False **kwargs: keyword arguments to pd.read_csv, np.loadtxt, dd.io.load, pickle, or open Returns: the loaded object \"\"\" if isinstance ( f , str ): f = Path ( f ) if not isinstance ( f , Path ): raise TypeError ( \"Input must be a string or Path object\" ) supported_exts = [ \".txt\" , \".json\" , \".p\" , \".pickle\" , \".csv\" , \".h5\" , \"hdf5\" , \".gz\" ] if f . suffix == \".csv\" : if verbose : print ( \"csv file - using pandas\" ) out = pd . read_csv ( str ( f ), ** kwargs ) elif f . suffix == \".h5\" or f . suffix == \".hdf5\" : if as_df : if verbose : print ( \"h5 file as df - using pandas\" ) out = pd . read_hdf ( str ( f ), key = h5_key ) else : if verbose : print ( \"h5 file - using deepdish\" ) out = dd . io . load ( str ( f )) elif f . suffix == \".txt\" : if as_arr : if verbose : print ( \"txt file - using numpy\" ) out = np . loadtxt ( str ( f )) else : if verbose : print ( \"txt file using - open\" ) with f . open () as file_handle : if as_str : out = file_handle . read () else : out = file_handle . readlines () elif f . suffix == \".p\" or f . suffix == \".pickle\" : if verbose : print ( \"pickle file - using pickle\" ) out = pickle . load ( open ( str ( f ), pickle_encoding )) elif f . suffix == \".json\" : if verbose : print ( \"json file - using pickle\" ) with f . open () as file_handle : if json_str : out = json . loads ( file_handle . read ()) else : out = json . load ( file_handle ) elif f . suffix == \".gz\" : if verbose : print ( \"gz file - using numpy\" ) out = np . loadtxt ( str ( f )) else : raise TypeError ( f \"File must end in one of: { supported_exts } \" ) return out save ( f ) A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file extension you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Parameters: Name Type Description Default f Union[pathlib.Path, str] complete filepath to save to including extension required Source code in utilz/io.py def save ( f : Union [ Path , str ]) -> None : \"\"\" A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file *extension* you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Args: f (Path/str): complete filepath to save to including extension \"\"\" pass","title":"Saving and Loading"},{"location":"api/io/#utilzio","text":"","title":"utilz.io"},{"location":"api/io/#utilz.io","text":"I/O Module for working with Paths","title":"utilz.io"},{"location":"api/io/#utilz.io.load","text":"A handy dandy all-in-one loading function. Simply pass a Path object to a file (or a string) and you'll back a python object. Supported extensions are: .txt, .csv, .json, .p, .pickle, .h5, .hdf5, .gz Parameters: Name Type Description Default f Union[pathlib.Path, str] name or path object to load required as_df bool treat a .hdf5, .h5 file as a Dataframe; Default False False as_arr bool treat a .txt file as a numpy array; False as_str bool open txt as a single string instead of False json_str bool treat a json file as a string (i.e use json.loads False h5_key str the key within the h5 file to load when if using as_df; 'data' pickle_encoding str pickle encoding to use; Default 'rb' 'rb' verbose bool whether to print messages during load. Default False False **kwargs keyword arguments to pd.read_csv, np.loadtxt, dd.io.load, pickle, or open {} Returns: Type Description Any the loaded object Source code in utilz/io.py def load ( f : Union [ Path , str ], as_df : bool = False , as_arr : bool = False , as_str : bool = False , h5_key : str = \"data\" , json_str : bool = False , pickle_encoding : str = \"rb\" , verbose : bool = False , ** kwargs , ) -> Any : \"\"\" A handy dandy all-in-one loading function. Simply pass a Path object to a file (or a string) and you'll back a python object. Supported extensions are: .txt, .csv, .json, .p, .pickle, .h5, .hdf5, .gz Args: f (Path/str): name or path object to load as_df (bool, optional): treat a .hdf5, .h5 file as a Dataframe; Default False as_arr (bool, optional): treat a .txt file as a numpy array; Default False as_str (bool, optional): open txt as a single string instead of splitting on newlines; Default False json_str (bool, optional): treat a json file as a string (i.e use json.loads instead of json.load); Default False h5_key (str, optional): the key within the h5 file to load when if using as_df; Default 'data' pickle_encoding (str, optional): pickle encoding to use; Default 'rb' verbose (bool, optional): whether to print messages during load. Default False **kwargs: keyword arguments to pd.read_csv, np.loadtxt, dd.io.load, pickle, or open Returns: the loaded object \"\"\" if isinstance ( f , str ): f = Path ( f ) if not isinstance ( f , Path ): raise TypeError ( \"Input must be a string or Path object\" ) supported_exts = [ \".txt\" , \".json\" , \".p\" , \".pickle\" , \".csv\" , \".h5\" , \"hdf5\" , \".gz\" ] if f . suffix == \".csv\" : if verbose : print ( \"csv file - using pandas\" ) out = pd . read_csv ( str ( f ), ** kwargs ) elif f . suffix == \".h5\" or f . suffix == \".hdf5\" : if as_df : if verbose : print ( \"h5 file as df - using pandas\" ) out = pd . read_hdf ( str ( f ), key = h5_key ) else : if verbose : print ( \"h5 file - using deepdish\" ) out = dd . io . load ( str ( f )) elif f . suffix == \".txt\" : if as_arr : if verbose : print ( \"txt file - using numpy\" ) out = np . loadtxt ( str ( f )) else : if verbose : print ( \"txt file using - open\" ) with f . open () as file_handle : if as_str : out = file_handle . read () else : out = file_handle . readlines () elif f . suffix == \".p\" or f . suffix == \".pickle\" : if verbose : print ( \"pickle file - using pickle\" ) out = pickle . load ( open ( str ( f ), pickle_encoding )) elif f . suffix == \".json\" : if verbose : print ( \"json file - using pickle\" ) with f . open () as file_handle : if json_str : out = json . loads ( file_handle . read ()) else : out = json . load ( file_handle ) elif f . suffix == \".gz\" : if verbose : print ( \"gz file - using numpy\" ) out = np . loadtxt ( str ( f )) else : raise TypeError ( f \"File must end in one of: { supported_exts } \" ) return out","title":"load()"},{"location":"api/io/#utilz.io.save","text":"A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file extension you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Parameters: Name Type Description Default f Union[pathlib.Path, str] complete filepath to save to including extension required Source code in utilz/io.py def save ( f : Union [ Path , str ]) -> None : \"\"\" A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file *extension* you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Args: f (Path/str): complete filepath to save to including extension \"\"\" pass","title":"save()"},{"location":"api/ops/","text":"utilz.ops Common data operations and transformations often on pandas dataframes norm_by_group ( df , grpcols , valcol , center = True , scale = True ) Normalize values in a column separately per group Parameters: Name Type Description Default df pd.DataFrame input dataframe required grpcols str/list grouping col(s) required valcol str value col required center bool mean center. Defaults to True. True scale bool divide by standard deviation. Defaults to True. True Source code in utilz/ops.py def norm_by_group ( df , grpcols , valcol , center = True , scale = True ): \"\"\" Normalize values in a column separately per group Args: df (pd.DataFrame): input dataframe grpcols (str/list): grouping col(s) valcol (str): value col center (bool, optional): mean center. Defaults to True. scale (bool, optional): divide by standard deviation. Defaults to True. \"\"\" def _norm ( dat , center , scale ): if center : dat = dat - dat . mean () if scale : dat = dat / dat . std () return dat return df . groupby ( grpcols )[ valcol ] . transform ( _norm , center , scale ) pmap ( func , iterme , func_args , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = 'processes' , progress = True , verbose = 0 , seed = None ) Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func Callable function to run required iterme Iterable an iterable for which each element will be passed to func required func_args list additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None required n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed Union[NoneType, int, numpy.random.mtrand.RandomState] random seed for reproducibility None Source code in utilz/ops.py def pmap ( func : Callable , iterme : Iterable , func_args : list , n_jobs : int = - 1 , loop_idx : bool = True , loop_random_seed : bool = False , backend : str = \"processes\" , progress : bool = True , verbose : int = 0 , seed : Union [ None , int , np . random . RandomState ] = None , ) -> Any : \"\"\" Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run iterme (iterable): an iterable for which each element will be passed to func func_args (list/dict/None): additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = len ( iterme )) if progress : iterator = tqdm ( len ( iterme )) else : iterator = len ( iterme ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func )( e , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args ) for e in iterme ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args ) for e in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out prep ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = 'processes' , progress = True , verbose = 0 , seed = None ) Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed int/None random seed for reproducibility None Examples: How to use a random seed. >>> from utilz.ops import ploop , random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum ( arr , seed = None ): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed ( seed ) >>> boot_arr = new_seed . choice ( arr , len ( arr ), replace = True ) >>> return boot_arr . sum () Finally call it in a parallel fashion >>> ploop ( boot_sum , [ np . arange ( 10 )], n_iter = 100 , loop_random_seed = True , loop_idx = False ) Source code in utilz/ops.py def prep ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = \"processes\" , progress = True , verbose = 0 , seed = None , ): \"\"\" Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility Examples: How to use a random seed. >>> from utilz.ops import ploop, random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum(arr, seed=None): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed(seed) >>> boot_arr = new_seed.choice(arr, len(arr), replace=True) >>> return boot_arr.sum() Finally call it in a parallel fashion >>> ploop(boot_sum, [np.arange(10)], n_iter=100, loop_random_seed=True, loop_idx=False) \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = n_iter ) if progress : iterator = tqdm ( range ( n_iter )) else : iterator = range ( n_iter ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func )( ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i }) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i }) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args ) for _ in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args ) for _ in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out random_seed ( seed ) Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state . Using the source here simply avoids an unecessary dependency. Parameters: Name Type Description Default seed None, int, np.RandomState iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. required Source code in utilz/ops.py def random_seed ( seed ): \"\"\"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to `sklearn.utils.check_random_state`. Using the source here simply avoids an unecessary dependency. Args: seed (None, int, np.RandomState): iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. \"\"\" import numbers if seed is None or seed is np . random : return np . random . mtrand . _rand if isinstance ( seed , ( numbers . Integral , np . integer )): return np . random . RandomState ( seed ) if isinstance ( seed , np . random . RandomState ): return seed raise ValueError ( \" %r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed )","title":"Operations"},{"location":"api/ops/#utilzops","text":"","title":"utilz.ops"},{"location":"api/ops/#utilz.ops","text":"Common data operations and transformations often on pandas dataframes","title":"utilz.ops"},{"location":"api/ops/#utilz.ops.norm_by_group","text":"Normalize values in a column separately per group Parameters: Name Type Description Default df pd.DataFrame input dataframe required grpcols str/list grouping col(s) required valcol str value col required center bool mean center. Defaults to True. True scale bool divide by standard deviation. Defaults to True. True Source code in utilz/ops.py def norm_by_group ( df , grpcols , valcol , center = True , scale = True ): \"\"\" Normalize values in a column separately per group Args: df (pd.DataFrame): input dataframe grpcols (str/list): grouping col(s) valcol (str): value col center (bool, optional): mean center. Defaults to True. scale (bool, optional): divide by standard deviation. Defaults to True. \"\"\" def _norm ( dat , center , scale ): if center : dat = dat - dat . mean () if scale : dat = dat / dat . std () return dat return df . groupby ( grpcols )[ valcol ] . transform ( _norm , center , scale )","title":"norm_by_group()"},{"location":"api/ops/#utilz.ops.pmap","text":"Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func Callable function to run required iterme Iterable an iterable for which each element will be passed to func required func_args list additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None required n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed Union[NoneType, int, numpy.random.mtrand.RandomState] random seed for reproducibility None Source code in utilz/ops.py def pmap ( func : Callable , iterme : Iterable , func_args : list , n_jobs : int = - 1 , loop_idx : bool = True , loop_random_seed : bool = False , backend : str = \"processes\" , progress : bool = True , verbose : int = 0 , seed : Union [ None , int , np . random . RandomState ] = None , ) -> Any : \"\"\" Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run iterme (iterable): an iterable for which each element will be passed to func func_args (list/dict/None): additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = len ( iterme )) if progress : iterator = tqdm ( len ( iterme )) else : iterator = len ( iterme ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func )( e , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args ) for e in iterme ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args ) for e in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out","title":"pmap()"},{"location":"api/ops/#utilz.ops.prep","text":"Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed int/None random seed for reproducibility None Examples: How to use a random seed. >>> from utilz.ops import ploop , random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum ( arr , seed = None ): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed ( seed ) >>> boot_arr = new_seed . choice ( arr , len ( arr ), replace = True ) >>> return boot_arr . sum () Finally call it in a parallel fashion >>> ploop ( boot_sum , [ np . arange ( 10 )], n_iter = 100 , loop_random_seed = True , loop_idx = False ) Source code in utilz/ops.py def prep ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = \"processes\" , progress = True , verbose = 0 , seed = None , ): \"\"\" Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility Examples: How to use a random seed. >>> from utilz.ops import ploop, random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum(arr, seed=None): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed(seed) >>> boot_arr = new_seed.choice(arr, len(arr), replace=True) >>> return boot_arr.sum() Finally call it in a parallel fashion >>> ploop(boot_sum, [np.arange(10)], n_iter=100, loop_random_seed=True, loop_idx=False) \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = n_iter ) if progress : iterator = tqdm ( range ( n_iter )) else : iterator = range ( n_iter ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func )( ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i }) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i }) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args ) for _ in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args ) for _ in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out","title":"prep()"},{"location":"api/ops/#utilz.ops.random_seed","text":"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state . Using the source here simply avoids an unecessary dependency. Parameters: Name Type Description Default seed None, int, np.RandomState iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. required Source code in utilz/ops.py def random_seed ( seed ): \"\"\"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to `sklearn.utils.check_random_state`. Using the source here simply avoids an unecessary dependency. Args: seed (None, int, np.RandomState): iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. \"\"\" import numbers if seed is None or seed is np . random : return np . random . mtrand . _rand if isinstance ( seed , ( numbers . Integral , np . integer )): return np . random . RandomState ( seed ) if isinstance ( seed , np . random . RandomState ): return seed raise ValueError ( \" %r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed )","title":"random_seed()"},{"location":"api/pipe/","text":"utilz.pipe utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok. Piping class similar to %>% in R. Pipe Pipe operator. Just initialize and assign to some variable to use immediately Example usage 1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe() # Some data to work with from seaborn import load_dataset df = load_dataset('iris') 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Pipe Operator"},{"location":"api/pipe/#utilzpipe","text":"utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok.","title":"utilz.pipe"},{"location":"api/pipe/#utilz.pipe","text":"Piping class similar to %>% in R.","title":"utilz.pipe"},{"location":"api/pipe/#utilz.pipe.Pipe","text":"Pipe operator. Just initialize and assign to some variable to use immediately","title":"Pipe"},{"location":"api/pipe/#example-usage","text":"1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe() # Some data to work with from seaborn import load_dataset df = load_dataset('iris') 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Example usage"},{"location":"api/plot/","text":"utilz.plot Plotting convenience functions setup ( figsize = ( 8 , 6 ), subplots = ( 1 , 1 )) Setup matplotlib subplots boilerplate Parameters: Name Type Description Default figsize tuple Figure size. Defaults to (8, 6). (8, 6) subplots tuple subplot grid size. Defaults to (1, 1). (1, 1) Returns: Type Description tuple ((Figure, Axes)) matplotlib figure handle and axes Source code in utilz/plot.py def setup ( figsize = ( 8 , 6 ), subplots = ( 1 , 1 )): \"\"\" Setup matplotlib subplots boilerplate Args: figsize (tuple, optional): Figure size. Defaults to (8, 6). subplots (tuple, optional): subplot grid size. Defaults to (1, 1). Returns: tuple ((Figure, Axes)): matplotlib figure handle and axes \"\"\" if \"plt\" not in dir (): import matplotlib.pyplot as plt f , ax = plt . subplots ( * subplots , figsize = figsize ) return f , ax","title":"Plotting Helpers"},{"location":"api/plot/#utilzplot","text":"","title":"utilz.plot"},{"location":"api/plot/#utilz.plot","text":"Plotting convenience functions","title":"utilz.plot"},{"location":"api/plot/#utilz.plot.setup","text":"Setup matplotlib subplots boilerplate Parameters: Name Type Description Default figsize tuple Figure size. Defaults to (8, 6). (8, 6) subplots tuple subplot grid size. Defaults to (1, 1). (1, 1) Returns: Type Description tuple ((Figure, Axes)) matplotlib figure handle and axes Source code in utilz/plot.py def setup ( figsize = ( 8 , 6 ), subplots = ( 1 , 1 )): \"\"\" Setup matplotlib subplots boilerplate Args: figsize (tuple, optional): Figure size. Defaults to (8, 6). subplots (tuple, optional): subplot grid size. Defaults to (1, 1). Returns: tuple ((Figure, Axes)): matplotlib figure handle and axes \"\"\" if \"plt\" not in dir (): import matplotlib.pyplot as plt f , ax = plt . subplots ( * subplots , figsize = figsize ) return f , ax","title":"setup()"},{"location":"api/termplot/","text":"utilz.termplot Plotting module dedicated to working with plots in an interactive terminal (not jupyter notebook!) init_termplot () Initilize terminal based plotting. Import and run this before any other python plotting module! e.g. before matplotlib. Requires the imgcat command line program available in Iterm. Source code in utilz/termplot.py def init_termplot (): \"\"\" Initilize terminal based plotting. **Import and run this before any other python plotting module!** e.g. before matplotlib. Requires the `imgcat` command line program available in Iterm. \"\"\" import matplotlib matplotlib . use ( \"module://imgcat\" ) p ( obj , * args , ** kwargs ) Show a plot in the terminal using an object's own .plot method. No need to call utilz.termplot.s() if using this function. Parameters: Name Type Description Default obj Any a python object that has a .plot() method required args Any arguments to the object's .plot() method () kwargs Any keyword arguments to the object's .plot() method {} Examples: >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}) >>> p ( df ) >>> f , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 3 )) >>> out = plot ( ax , [ 1 , 2 , 3 ]) Source code in utilz/termplot.py def p ( obj , * args , ** kwargs ): \"\"\" Show a plot in the terminal using an object's own .plot method. No need to call `utilz.termplot.s()` if using this function. Args: obj (Any): a python object that has a `.plot()` method args (Any): arguments to the object's `.plot()` method kwargs (Any): keyword arguments to the object's `.plot()` method Examples: >>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}) >>> p(df) >>> f, ax = plt.subplots(1, 1, figsize=(4, 3)) >>> out = plot(ax, [1, 2, 3]) \"\"\" plot = getattr ( obj , \"plot\" , None ) if callable ( plot ): out = obj . plot ( * args , ** kwargs ) s () return out else : raise TypeError ( f \"Object of type { type ( obj ) } has not .plot() method!\" ) s () Show a plot in the terminal and immediately close the figure handle to save memory. This has to be called immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using utilz.termplot.p() , which will automatically call this function Examples: >>> plt . plot ([ 1 , 2 , 3 ]) >>> s () >>> sns . scatterplot ( 'x' , 'y' , data = df ) >>> s () Source code in utilz/termplot.py def s (): \"\"\" Show a plot in the terminal and immediately close the figure handle to save memory. This **has to be called** immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using `utilz.termplot.p()`, which will automatically call this function Examples: >>> plt.plot([1,2,3]) >>> s() >>> sns.scatterplot('x','y',data=df) >>> s() \"\"\" if \"plt\" not in dir (): import matplotlib.pyplot as plt if len ( plt . get_fignums ()): plt . show () plt . close () else : raise ValueError ( \"No matplotlib figures found. Are you sure you plotted something?\" )","title":"Terminal Plots"},{"location":"api/termplot/#utilztermplot","text":"","title":"utilz.termplot"},{"location":"api/termplot/#utilz.termplot","text":"Plotting module dedicated to working with plots in an interactive terminal (not jupyter notebook!)","title":"utilz.termplot"},{"location":"api/termplot/#utilz.termplot.init_termplot","text":"Initilize terminal based plotting. Import and run this before any other python plotting module! e.g. before matplotlib. Requires the imgcat command line program available in Iterm. Source code in utilz/termplot.py def init_termplot (): \"\"\" Initilize terminal based plotting. **Import and run this before any other python plotting module!** e.g. before matplotlib. Requires the `imgcat` command line program available in Iterm. \"\"\" import matplotlib matplotlib . use ( \"module://imgcat\" )","title":"init_termplot()"},{"location":"api/termplot/#utilz.termplot.p","text":"Show a plot in the terminal using an object's own .plot method. No need to call utilz.termplot.s() if using this function. Parameters: Name Type Description Default obj Any a python object that has a .plot() method required args Any arguments to the object's .plot() method () kwargs Any keyword arguments to the object's .plot() method {} Examples: >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}) >>> p ( df ) >>> f , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 3 )) >>> out = plot ( ax , [ 1 , 2 , 3 ]) Source code in utilz/termplot.py def p ( obj , * args , ** kwargs ): \"\"\" Show a plot in the terminal using an object's own .plot method. No need to call `utilz.termplot.s()` if using this function. Args: obj (Any): a python object that has a `.plot()` method args (Any): arguments to the object's `.plot()` method kwargs (Any): keyword arguments to the object's `.plot()` method Examples: >>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}) >>> p(df) >>> f, ax = plt.subplots(1, 1, figsize=(4, 3)) >>> out = plot(ax, [1, 2, 3]) \"\"\" plot = getattr ( obj , \"plot\" , None ) if callable ( plot ): out = obj . plot ( * args , ** kwargs ) s () return out else : raise TypeError ( f \"Object of type { type ( obj ) } has not .plot() method!\" )","title":"p()"},{"location":"api/termplot/#utilz.termplot.s","text":"Show a plot in the terminal and immediately close the figure handle to save memory. This has to be called immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using utilz.termplot.p() , which will automatically call this function Examples: >>> plt . plot ([ 1 , 2 , 3 ]) >>> s () >>> sns . scatterplot ( 'x' , 'y' , data = df ) >>> s () Source code in utilz/termplot.py def s (): \"\"\" Show a plot in the terminal and immediately close the figure handle to save memory. This **has to be called** immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using `utilz.termplot.p()`, which will automatically call this function Examples: >>> plt.plot([1,2,3]) >>> s() >>> sns.scatterplot('x','y',data=df) >>> s() \"\"\" if \"plt\" not in dir (): import matplotlib.pyplot as plt if len ( plt . get_fignums ()): plt . show () plt . close () else : raise ValueError ( \"No matplotlib figures found. Are you sure you plotted something?\" )","title":"s()"}]}