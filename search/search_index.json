{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Utilz A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Home"},{"location":"#utilz","text":"A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Utilz"},{"location":"api/guards/","text":"Guards and Decorators Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators, e.g. @log_df def myfunc(df): do some stuff... disk_cache ( threshold = 60 , autoload = True , index = False , verbose = False ) Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 60. 60 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool; optional whether to incluce the index when saving a dataframe. Default to False False verbose bool; optional print debug messages; Default to False False Source code in utilz/guards.py def disk_cache ( threshold = 60 , autoload = True , index = False , verbose = False ): \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 60. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False verbose (bool; optional): print debug messages; Default to False \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): filtered_args = [ e for e in args if not isinstance ( e , ( list , pd . DataFrame , np . ndarray )) ] key = str (( filtered_args , tuple ( sorted ( kwargs . items ())))) key_csv = f \" { func . __name__ } _ { key } .csv\" key_h5 = f \" { func . __name__ } _ { key } .h5\" if autoload : if Path ( key_csv ) . exists (): if verbose : print ( \"Returning cached result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): if verbose : print ( \"Returning cached result\" ) return dd . io . load ( key_h5 ) if verbose : print ( \"No cached result...executing\" ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): fname = f \" { func . __name__ } _ { key } .csv\" result . to_csv ( fname , index = index ) else : fname = f \" { func . __name__ } _ { key } .h5\" dd . io . save ( fname , result , compression = \"zlib\" ) print ( f \"Exceeded threshold. Result cached to { fname } \" ) return result return wrapper return decorator log_df ( func ) Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper","title":"Guards and Decorators"},{"location":"api/guards/#guards-and-decorators","text":"","title":"Guards and Decorators"},{"location":"api/guards/#utilz.guards","text":"Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators, e.g. @log_df def myfunc(df): do some stuff...","title":"utilz.guards"},{"location":"api/guards/#utilz.guards.disk_cache","text":"Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 60. 60 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool; optional whether to incluce the index when saving a dataframe. Default to False False verbose bool; optional print debug messages; Default to False False Source code in utilz/guards.py def disk_cache ( threshold = 60 , autoload = True , index = False , verbose = False ): \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same arrangement of args and kwargs, first try to load the last result and return that, rather than rerunning the function. Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 60. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False verbose (bool; optional): print debug messages; Default to False \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): filtered_args = [ e for e in args if not isinstance ( e , ( list , pd . DataFrame , np . ndarray )) ] key = str (( filtered_args , tuple ( sorted ( kwargs . items ())))) key_csv = f \" { func . __name__ } _ { key } .csv\" key_h5 = f \" { func . __name__ } _ { key } .h5\" if autoload : if Path ( key_csv ) . exists (): if verbose : print ( \"Returning cached result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): if verbose : print ( \"Returning cached result\" ) return dd . io . load ( key_h5 ) if verbose : print ( \"No cached result...executing\" ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): fname = f \" { func . __name__ } _ { key } .csv\" result . to_csv ( fname , index = index ) else : fname = f \" { func . __name__ } _ { key } .h5\" dd . io . save ( fname , result , compression = \"zlib\" ) print ( f \"Exceeded threshold. Result cached to { fname } \" ) return result return wrapper return decorator","title":"disk_cache()"},{"location":"api/guards/#utilz.guards.log_df","text":"Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper","title":"log_df()"},{"location":"api/ops/","text":"Operations Common data operations and transformations. Often on pandas dataframes MAX_INT ploop ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = 'processes' , progress = True , verbose = 0 , seed = None ) Call a function for n_iter using parallelization via joblib Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed int/None random seed for reproducibility None Examples: How to use a random seed. >>> from utilz.ops import ploop , random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum ( arr , seed = None ): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed ( seed ) >>> boot_arr = new_seed . choice ( arr , len ( arr ), replace = True ) >>> return boot_arr . sum () Finally call it in a parallel fashion >>> ploop ( boot_sum , [ np . arange ( 10 )], n_iter = 100 , loop_random_seed = True , loop_idx = False ) Source code in utilz/ops.py def ploop ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = \"processes\" , progress = True , verbose = 0 , seed = None , ): \"\"\" Call a function for n_iter using parallelization via joblib Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility Examples: How to use a random seed. >>> from utilz.ops import ploop, random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum(arr, seed=None): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed(seed) >>> boot_arr = new_seed.choice(arr, len(arr), replace=True) >>> return boot_arr.sum() Finally call it in a parallel fashion >>> ploop(boot_sum, [np.arange(10)], n_iter=100, loop_random_seed=True, loop_idx=False) \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = n_iter ) if progress : iterator = tqdm ( range ( n_iter )) else : iterator = range ( n_iter ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func )( ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func ) for i in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i }) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args ) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args ) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out random_seed ( seed ) Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state. Using the source here simply avoids an unecessary dependency. Parameters: Name Type Description Default seed None, int, np.RandomState iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. required Source code in utilz/ops.py def random_seed ( seed ): \"\"\"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state. Using the source here simply avoids an unecessary dependency. Args: seed (None, int, np.RandomState): iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. \"\"\" import numbers if seed is None or seed is np . random : return np . random . mtrand . _rand if isinstance ( seed , ( numbers . Integral , np . integer )): return np . random . RandomState ( seed ) if isinstance ( seed , np . random . RandomState ): return seed raise ValueError ( \" %r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed )","title":"Operations"},{"location":"api/ops/#operations","text":"","title":"Operations"},{"location":"api/ops/#utilz.ops","text":"Common data operations and transformations. Often on pandas dataframes","title":"utilz.ops"},{"location":"api/ops/#utilz.ops.MAX_INT","text":"","title":"MAX_INT"},{"location":"api/ops/#utilz.ops.ploop","text":"Call a function for n_iter using parallelization via joblib Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed int/None random seed for reproducibility None Examples: How to use a random seed. >>> from utilz.ops import ploop , random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum ( arr , seed = None ): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed ( seed ) >>> boot_arr = new_seed . choice ( arr , len ( arr ), replace = True ) >>> return boot_arr . sum () Finally call it in a parallel fashion >>> ploop ( boot_sum , [ np . arange ( 10 )], n_iter = 100 , loop_random_seed = True , loop_idx = False ) Source code in utilz/ops.py def ploop ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = \"processes\" , progress = True , verbose = 0 , seed = None , ): \"\"\" Call a function for n_iter using parallelization via joblib Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility Examples: How to use a random seed. >>> from utilz.ops import ploop, random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum(arr, seed=None): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed(seed) >>> boot_arr = new_seed.choice(arr, len(arr), replace=True) >>> return boot_arr.sum() Finally call it in a parallel fashion >>> ploop(boot_sum, [np.arange(10)], n_iter=100, loop_random_seed=True, loop_idx=False) \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = n_iter ) if progress : iterator = tqdm ( range ( n_iter )) else : iterator = range ( n_iter ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func )( ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func ) for i in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i }) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args ) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args ) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out","title":"ploop()"},{"location":"api/ops/#utilz.ops.random_seed","text":"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state. Using the source here simply avoids an unecessary dependency. Parameters: Name Type Description Default seed None, int, np.RandomState iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. required Source code in utilz/ops.py def random_seed ( seed ): \"\"\"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state. Using the source here simply avoids an unecessary dependency. Args: seed (None, int, np.RandomState): iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. \"\"\" import numbers if seed is None or seed is np . random : return np . random . mtrand . _rand if isinstance ( seed , ( numbers . Integral , np . integer )): return np . random . RandomState ( seed ) if isinstance ( seed , np . random . RandomState ): return seed raise ValueError ( \" %r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed )","title":"random_seed()"},{"location":"api/pipe/","text":"Pipe utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok. Example usage 1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe() # Some data to work with from seaborn import load_dataset df = load_dataset('iris') 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Pipe Operator"},{"location":"api/pipe/#pipe","text":"utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok.","title":"Pipe"},{"location":"api/pipe/#example-usage","text":"1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe() # Some data to work with from seaborn import load_dataset df = load_dataset('iris') 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Example usage"}]}