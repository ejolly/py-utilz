{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Utilz A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Home"},{"location":"#utilz","text":"A python package that combines several key ideas to make data analysis faster, easier, and more reliable: Defensive data analysis through the likes of bulwark Minimization of boilerplate code, e.g. for plotting in matplotlib and seaborn Common data operations in pandas, e.g. normalizing by group Common i/o operations like managing paths More concise and reusable code via functional programming ideas via toolz , e.g. >>o>> as a pipe operator","title":"Utilz"},{"location":"api/goodpipes/","text":"Efficient functional-programming-style data analysis from utilz import clear_cache , disk_cache , load from toolz import memoize , curry , pipe First clear any existing cache, i.e. the .utilz_cache folder. clear_cache () Memoization Memoize a function to save its last input in RAM and recall it when called with the same inputs rather than re-executing a potentially long running function @memoize def load ( path ): \"Simulate loading a slow large dataset\" print ( \"loading from disk\" ) sleep ( 5 ) return pd . read_csv ( path ) Partial function application (currying) Curry so a function operates normally when it receives all its required arguments, but turns into a partial function when it gets fewer that all its required arguments. This partial function behaves just like the original except with a subset of its arguments fixed. This also allows any functions to work with toolz.pipe() , while still being able to manipulate a function's arguments. That's because pipe() implicitly passes the output of the last function as the input of the next. @curry def calc_mean ( df , normalize = False ): ... # With no kwargs you sometimes have to write the args backwards @curry def calc_mean ( norm_value , df ): ... # Now this works, otherwise it would complain about the wrong number of arguments pipe ( df , calc_mean ( normalize = True ) ) Cacheing outputs to disk Cache so the result of a function is stored to disk in file made unique by the args and kwargs to the function. Use utilz.disk_cache to decorate a function so it caches, which works just like toolz.memoize but stores the result to a file and loads the file when called with the same inputs. Better that memoize for larger memory hungry inputs and outputs, and necessary if input or outputs cannot be pickled (e.g. dataframes, arrays, deep objects, etc). Setting the threshold to something like 1 or 0 essentially always caches the result. @curry @disk_cache ( threshold = 1 ) def norm ( df , num = '' , denom = '' ): \"Simulate expensive normalization function that takes args\" print ( \"computing...\" ) sleep ( 5 ) return pd . DataFrame ({ \"norm\" : df [ num ] / df [ denom ]}) Together Because load is memoized by default, only the first run of this pipeline actually loads the data and incurs the i/o cost. Because norm is decorated by disk_cache , only the first run with the same prior pipeline steps incurs a compute cost. summary = pipe ( \"test.csv\" , load , groupby ( \"D\" ), assign ( a_centered = \"A - B.mean()\" , mean_C = \"mean(C)\" , std_C = \"std(C)\" ), norm ( num = 'A' , denom = 'B' ) ) # Using the joblib.Memory module also does disk-cacheing, but # for some reason doesn't seem to work with currying This setup is nice because it allows for both interactive data analysis as well as reproducible scripts. Simply start writing the pipeline steps, and comment out ones you want to skip or debug. In another notebook cell edit the source code of a function in the pipeline and incrementally add to its body, while rerunning the pipeline to see results as you build up your functions. For functions that take a while to run, try wrapping them in memoize or disk_cache . Memoize is nice for loading csv/text files (so you don't need to re-read them from disk each re-run of the pipeline). Cacheing is nice for expensive operations or operations on complex datastructures like arrays and dataframes. Plus, utilz saves them in standard robust file types ( .csv . or . h5 ) files so you're also getting incremental backups of your work. No more need to rely on saved \"state\" in a Juptyer notebook.","title":"Functional-programming-style data analysis"},{"location":"api/goodpipes/#efficient-functional-programming-style-data-analysis","text":"from utilz import clear_cache , disk_cache , load from toolz import memoize , curry , pipe First clear any existing cache, i.e. the .utilz_cache folder. clear_cache ()","title":"Efficient functional-programming-style data analysis"},{"location":"api/goodpipes/#memoization","text":"Memoize a function to save its last input in RAM and recall it when called with the same inputs rather than re-executing a potentially long running function @memoize def load ( path ): \"Simulate loading a slow large dataset\" print ( \"loading from disk\" ) sleep ( 5 ) return pd . read_csv ( path )","title":"Memoization"},{"location":"api/goodpipes/#partial-function-application-currying","text":"Curry so a function operates normally when it receives all its required arguments, but turns into a partial function when it gets fewer that all its required arguments. This partial function behaves just like the original except with a subset of its arguments fixed. This also allows any functions to work with toolz.pipe() , while still being able to manipulate a function's arguments. That's because pipe() implicitly passes the output of the last function as the input of the next. @curry def calc_mean ( df , normalize = False ): ... # With no kwargs you sometimes have to write the args backwards @curry def calc_mean ( norm_value , df ): ... # Now this works, otherwise it would complain about the wrong number of arguments pipe ( df , calc_mean ( normalize = True ) )","title":"Partial function application (currying)"},{"location":"api/goodpipes/#cacheing-outputs-to-disk","text":"Cache so the result of a function is stored to disk in file made unique by the args and kwargs to the function. Use utilz.disk_cache to decorate a function so it caches, which works just like toolz.memoize but stores the result to a file and loads the file when called with the same inputs. Better that memoize for larger memory hungry inputs and outputs, and necessary if input or outputs cannot be pickled (e.g. dataframes, arrays, deep objects, etc). Setting the threshold to something like 1 or 0 essentially always caches the result. @curry @disk_cache ( threshold = 1 ) def norm ( df , num = '' , denom = '' ): \"Simulate expensive normalization function that takes args\" print ( \"computing...\" ) sleep ( 5 ) return pd . DataFrame ({ \"norm\" : df [ num ] / df [ denom ]})","title":"Cacheing outputs to disk"},{"location":"api/goodpipes/#together","text":"Because load is memoized by default, only the first run of this pipeline actually loads the data and incurs the i/o cost. Because norm is decorated by disk_cache , only the first run with the same prior pipeline steps incurs a compute cost. summary = pipe ( \"test.csv\" , load , groupby ( \"D\" ), assign ( a_centered = \"A - B.mean()\" , mean_C = \"mean(C)\" , std_C = \"std(C)\" ), norm ( num = 'A' , denom = 'B' ) ) # Using the joblib.Memory module also does disk-cacheing, but # for some reason doesn't seem to work with currying This setup is nice because it allows for both interactive data analysis as well as reproducible scripts. Simply start writing the pipeline steps, and comment out ones you want to skip or debug. In another notebook cell edit the source code of a function in the pipeline and incrementally add to its body, while rerunning the pipeline to see results as you build up your functions. For functions that take a while to run, try wrapping them in memoize or disk_cache . Memoize is nice for loading csv/text files (so you don't need to re-read them from disk each re-run of the pipeline). Cacheing is nice for expensive operations or operations on complex datastructures like arrays and dataframes. Plus, utilz saves them in standard robust file types ( .csv . or . h5 ) files so you're also getting incremental backups of your work. No more need to rely on saved \"state\" in a Juptyer notebook.","title":"Together"},{"location":"api/guards/","text":"utilz.guards Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators: from utilz.guards import log_df @log_df def myfunc(df): do some stuff... disk_cache ( threshold = 30 , autoload = True , index = False , save_dir = '.utilz_cache' ) Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to .utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5} . Very similar in spirit to @memory.cache decorator in joblib but instead uses csv's to persist Dataframes and hd5f to persist everything else rather than pickles. Also works better in combination with the @curry decorator from toolz/cytoolz Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 30. 30 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool whether to incluce the index when saving a dataframe. Default to False False save_dir str location to cache results; Default '.utilz_cache' '.utilz_cache' Source code in utilz/guards.py def disk_cache ( threshold : int = 30 , autoload : bool = True , index : bool = False , save_dir : str = \".utilz_cache\" , ) -> Any : \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to `.utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5}`. Very similar in spirit to @memory.cache decorator in joblib but instead uses csv's to persist Dataframes and hd5f to persist everything else rather than pickles. Also works better in combination with the @curry decorator from toolz/cytoolz Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 30. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False save_dir (str; optional): location to cache results; Default '.utilz_cache' \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): saved_inputs = dict ( sorted ( getcallargs ( func , * args , ** kwargs ) . items ())) cache_dir = Path ( save_dir ) if not cache_dir . exists (): cache_dir . mkdir () inputs = {} for k , v in saved_inputs . items (): if k == \"args\" : new_v = [ _hashobj ( e ) for e in v ] inputs [ k ] = new_v elif k == \"kwargs\" : new_v = { kk : _hashobj ( vv ) for kk , vv in v . items ()} inputs [ k ] = new_v else : inputs [ k ] = _hashobj ( v ) key = ( dumps ( inputs ) . replace ( '\"' , \"\" ) . replace ( \"{\" , \"\" ) . replace ( \"}\" , \"\" ) . replace ( \"[\" , \"\" ) . replace ( \"]\" , \"\" ) . replace ( \":\" , \"__\" ) . replace ( \" \" , \"\" ) . replace ( \",\" , \"--\" ) ) key_csv = f \" { func . __name__ } ___ { key } .csv\" key_csv = cache_dir . joinpath ( key_csv ) key_h5 = f \" { func . __name__ } ___ { key } .h5\" key_h5 = cache_dir . joinpath ( key_h5 ) if autoload : if Path ( key_csv ) . exists (): print ( f \"Returning { func . __name__ } cached result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): print ( f \"Returning { func . __name__ } cached result\" ) return dd . io . load ( key_h5 ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): result . to_csv ( str ( key_csv ), index = index ) print ( f \"Exceeded compute time. Result cached to { key_csv } \" ) else : dd . io . save ( str ( key_h5 ), result , compression = \"zlib\" ) print ( f \"Exceeded compute time. Result cached to { key_h5 } \" ) return result return wrapper return decorator log ( func ) Log the type and shape/size/len of the output from a function Parameters: Name Type Description Default func callable any pure function (i.e, has no side-effects) required Source code in utilz/guards.py def log ( func ): \"\"\" Log the type and shape/size/len of the output from a function Args: func (callable): any pure function (i.e, has no side-effects) \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper log_df ( func ) Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper maybe ( filepath , force = False ) Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Parameters: Name Type Description Default filepath Union[pathlib.Path, str] filename or path to check existence for required force bool always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. False Source code in utilz/guards.py def maybe ( filepath : Union [ Path , str ], force : bool = False ) -> Any : \"\"\" Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Args: filepath (Path/str): filename or path to check existence for force (bool, optional): always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): fpath = Path ( filepath ) if not force : if fpath . exists (): return load ( fpath ) else : return func ( * args , ** kwargs ) else : return func ( * args , ** kwargs ) return wrapper return decorator same_nunique ( func , val_col , group_col ) Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required val_col str column name to check for unique values in dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_nunique ( func , val_col , group_col ): \"\"\" Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Args: func (callable): a function that operates on a dataframe val_col (str): column name to check for unique values in dataframe group_call (str): column name to group on in dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper same_shape ( grpcols , shape = None ) Check if each group of group_col has the same dimensions after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_shape ( grpcols , shape = None ): \"\"\" Check if each group of group_col has the same dimensions after running a function on a dataframe Args: func (callable): a function that operates on a dataframe group_call (str): column name to group on in dataframe \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): df = func ( * args , ** kwargs ) grouped = df . groupby ( grpcols ) . size () if shape is None : if not grouped . sum () % grouped . shape [ 0 ] == 0 : raise AssertionError ( \"Groups dont have the same shape\" , grouped ) else : if not all ( grouped == shape ): raise AssertionError ( f \"All groups dont match shape { shape } \" , grouped ) return df return wrapper return decorator","title":"Guards and Decorators"},{"location":"api/guards/#utilzguards","text":"","title":"utilz.guards"},{"location":"api/guards/#utilz.guards","text":"Custom guards for defensive data analysis compatible with bulwark . Intended usage is as Python decorators: from utilz.guards import log_df @log_df def myfunc(df): do some stuff...","title":"utilz.guards"},{"location":"api/guards/#utilz.guards.disk_cache","text":"Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to .utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5} . Very similar in spirit to @memory.cache decorator in joblib but instead uses csv's to persist Dataframes and hd5f to persist everything else rather than pickles. Also works better in combination with the @curry decorator from toolz/cytoolz Parameters: Name Type Description Default threshold int threshold in seconds over which object is saved to disk. Defaults to 30. 30 autoload bool whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; True index bool whether to incluce the index when saving a dataframe. Default to False False save_dir str location to cache results; Default '.utilz_cache' '.utilz_cache' Source code in utilz/guards.py def disk_cache ( threshold : int = 30 , autoload : bool = True , index : bool = False , save_dir : str = \".utilz_cache\" , ) -> Any : \"\"\" Save the result of a function to disk if it takes longer than threshold to run. Then on subsequent runs given the same args and kwargs, first try to load the last result and return that, rather than rerunning the function, i.e. processing-time based memoization. The resulting file is saved to `.utilz_cache/funcname___arg1__arg1val--arg2__arg2val__kwarg1__kwarg1val--kwarg2__kwarg2val.{csv/h5}`. Very similar in spirit to @memory.cache decorator in joblib but instead uses csv's to persist Dataframes and hd5f to persist everything else rather than pickles. Also works better in combination with the @curry decorator from toolz/cytoolz Args: threshold (int, optional): threshold in seconds over which object is saved to disk. Defaults to 30. autoload (bool, optional): whether to try to load a previously persisted result if all args and kwargs match in a subsequent function call. Default to True; index (bool; optional): whether to incluce the index when saving a dataframe. Default to False save_dir (str; optional): location to cache results; Default '.utilz_cache' \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): saved_inputs = dict ( sorted ( getcallargs ( func , * args , ** kwargs ) . items ())) cache_dir = Path ( save_dir ) if not cache_dir . exists (): cache_dir . mkdir () inputs = {} for k , v in saved_inputs . items (): if k == \"args\" : new_v = [ _hashobj ( e ) for e in v ] inputs [ k ] = new_v elif k == \"kwargs\" : new_v = { kk : _hashobj ( vv ) for kk , vv in v . items ()} inputs [ k ] = new_v else : inputs [ k ] = _hashobj ( v ) key = ( dumps ( inputs ) . replace ( '\"' , \"\" ) . replace ( \"{\" , \"\" ) . replace ( \"}\" , \"\" ) . replace ( \"[\" , \"\" ) . replace ( \"]\" , \"\" ) . replace ( \":\" , \"__\" ) . replace ( \" \" , \"\" ) . replace ( \",\" , \"--\" ) ) key_csv = f \" { func . __name__ } ___ { key } .csv\" key_csv = cache_dir . joinpath ( key_csv ) key_h5 = f \" { func . __name__ } ___ { key } .h5\" key_h5 = cache_dir . joinpath ( key_h5 ) if autoload : if Path ( key_csv ) . exists (): print ( f \"Returning { func . __name__ } cached result\" ) return pd . read_csv ( key_csv ) elif Path ( key_h5 ) . exists (): print ( f \"Returning { func . __name__ } cached result\" ) return dd . io . load ( key_h5 ) tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = dt . datetime . now () - tic if time_taken . seconds > threshold : if isinstance ( result , pd . DataFrame ): result . to_csv ( str ( key_csv ), index = index ) print ( f \"Exceeded compute time. Result cached to { key_csv } \" ) else : dd . io . save ( str ( key_h5 ), result , compression = \"zlib\" ) print ( f \"Exceeded compute time. Result cached to { key_h5 } \" ) return result return wrapper return decorator","title":"disk_cache()"},{"location":"api/guards/#utilz.guards.log","text":"Log the type and shape/size/len of the output from a function Parameters: Name Type Description Default func callable any pure function (i.e, has no side-effects) required Source code in utilz/guards.py def log ( func ): \"\"\" Log the type and shape/size/len of the output from a function Args: func (callable): any pure function (i.e, has no side-effects) \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper","title":"log()"},{"location":"api/guards/#utilz.guards.log_df","text":"Log the shape and run time of a function that operates on a pandas dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required Source code in utilz/guards.py def log_df ( func ): \"\"\" Log the shape and run time of a function that operates on a pandas dataframe Args: func (callable): a function that operates on a dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): tic = dt . datetime . now () result = func ( * args , ** kwargs ) time_taken = str ( dt . datetime . now () - tic ) print ( f \"Func { func . __name__ } df shape= { result . shape } took { time_taken } s\" ) return result return wrapper","title":"log_df()"},{"location":"api/guards/#utilz.guards.maybe","text":"Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Parameters: Name Type Description Default filepath Union[pathlib.Path, str] filename or path to check existence for required force bool always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. False Source code in utilz/guards.py def maybe ( filepath : Union [ Path , str ], force : bool = False ) -> Any : \"\"\" Run the decorated func only if filepath doesn't exist. Override to always run the function with force. Args: filepath (Path/str): filename or path to check existence for force (bool, optional): always run the function even if filepath exists, possibly overwriting filepath (based on whatever func does internally. Defaults to False. \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): fpath = Path ( filepath ) if not force : if fpath . exists (): return load ( fpath ) else : return func ( * args , ** kwargs ) else : return func ( * args , ** kwargs ) return wrapper return decorator","title":"maybe()"},{"location":"api/guards/#utilz.guards.same_nunique","text":"Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required val_col str column name to check for unique values in dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_nunique ( func , val_col , group_col ): \"\"\" Check if each group of group_col has the same number of unique values of val_col after running a function on a dataframe Args: func (callable): a function that operates on a dataframe val_col (str): column name to check for unique values in dataframe group_call (str): column name to group on in dataframe \"\"\" @wraps ( func ) def wrapper ( * args , ** kwargs ): pass return wrapper","title":"same_nunique()"},{"location":"api/guards/#utilz.guards.same_shape","text":"Check if each group of group_col has the same dimensions after running a function on a dataframe Parameters: Name Type Description Default func callable a function that operates on a dataframe required group_call str column name to group on in dataframe required Source code in utilz/guards.py def same_shape ( grpcols , shape = None ): \"\"\" Check if each group of group_col has the same dimensions after running a function on a dataframe Args: func (callable): a function that operates on a dataframe group_call (str): column name to group on in dataframe \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): df = func ( * args , ** kwargs ) grouped = df . groupby ( grpcols ) . size () if shape is None : if not grouped . sum () % grouped . shape [ 0 ] == 0 : raise AssertionError ( \"Groups dont have the same shape\" , grouped ) else : if not all ( grouped == shape ): raise AssertionError ( f \"All groups dont match shape { shape } \" , grouped ) return df return wrapper return decorator","title":"same_shape()"},{"location":"api/io/","text":"utilz.io I/O Module for working with Paths nbload ( varname , fname , to_arr = False ) Load a variable previously saved in a notebook's json meta-data using scrapbook Parameters: Name Type Description Default varname string variable to load required fname string/Path notebook path to load required to_arr bool whether to cast the loaded variable to a numpy array; Default False False Returns: Type Description Any the loaded variable Source code in utilz/io.py def nbload ( varname , fname , to_arr = False ): \"\"\" Load a variable previously saved in a notebook's json meta-data using scrapbook Args: varname (string): variable to load fname (string/Path): notebook path to load to_arr (bool, optional): whether to cast the loaded variable to a numpy array; Default False Returns: Any: the loaded variable \"\"\" if isinstance ( fname , Path ): fname = str ( Path ) if to_arr : return np . array ( sb . read_notebook ( fname ) . scraps [ varname ] . data ) else : return sb . read_notebook ( fname ) . scraps [ varname ] . data nbsave ( var , varname ) Save a variable within a notebook's json meta-data using scrapbook. Auto-converts numpy arrays to lists before saving Parameters: Name Type Description Default var Any variable to save required varname string what to call the variable in the notebook meta-data required Source code in utilz/io.py def nbsave ( var , varname ): \"\"\" Save a variable within a notebook's json meta-data using scrapbook. Auto-converts numpy arrays to lists before saving Args: var (Any): variable to save varname (string): what to call the variable in the notebook meta-data \"\"\" if isinstance ( var , np . ndarray ): var = var . tolist () sb . glue ( varname , var ) save ( f ) A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file extension you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Parameters: Name Type Description Default f Union[pathlib.Path, str] complete filepath to save to including extension required Source code in utilz/io.py def save ( f : Union [ Path , str ]) -> None : \"\"\" A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file *extension* you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Args: f (Path/str): complete filepath to save to including extension \"\"\" pass","title":"Saving and Loading"},{"location":"api/io/#utilzio","text":"","title":"utilz.io"},{"location":"api/io/#utilz.io","text":"I/O Module for working with Paths","title":"utilz.io"},{"location":"api/io/#utilz.io.nbload","text":"Load a variable previously saved in a notebook's json meta-data using scrapbook Parameters: Name Type Description Default varname string variable to load required fname string/Path notebook path to load required to_arr bool whether to cast the loaded variable to a numpy array; Default False False Returns: Type Description Any the loaded variable Source code in utilz/io.py def nbload ( varname , fname , to_arr = False ): \"\"\" Load a variable previously saved in a notebook's json meta-data using scrapbook Args: varname (string): variable to load fname (string/Path): notebook path to load to_arr (bool, optional): whether to cast the loaded variable to a numpy array; Default False Returns: Any: the loaded variable \"\"\" if isinstance ( fname , Path ): fname = str ( Path ) if to_arr : return np . array ( sb . read_notebook ( fname ) . scraps [ varname ] . data ) else : return sb . read_notebook ( fname ) . scraps [ varname ] . data","title":"nbload()"},{"location":"api/io/#utilz.io.nbsave","text":"Save a variable within a notebook's json meta-data using scrapbook. Auto-converts numpy arrays to lists before saving Parameters: Name Type Description Default var Any variable to save required varname string what to call the variable in the notebook meta-data required Source code in utilz/io.py def nbsave ( var , varname ): \"\"\" Save a variable within a notebook's json meta-data using scrapbook. Auto-converts numpy arrays to lists before saving Args: var (Any): variable to save varname (string): what to call the variable in the notebook meta-data \"\"\" if isinstance ( var , np . ndarray ): var = var . tolist () sb . glue ( varname , var )","title":"nbsave()"},{"location":"api/io/#utilz.io.save","text":"A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file extension you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Parameters: Name Type Description Default f Union[pathlib.Path, str] complete filepath to save to including extension required Source code in utilz/io.py def save ( f : Union [ Path , str ]) -> None : \"\"\" A handy dandy all-in-one saving function. Simply pass a Path object to a file (or a string) and it will be saved based upon the file *extension* you provide. Suported extensions are : .txt, .csv, .json, .p, .pickle, .h5, .hd5f, .gz Args: f (Path/str): complete filepath to save to including extension \"\"\" pass","title":"save()"},{"location":"api/ops/","text":"utilz.ops Common data operations and transformations often on pandas dataframes apply ( func , iterme , as_df = False , as_arr = False , axis = 0 , ignore_index = True ) Applys func to iterme and combines the result into a single, list, dataFrame or array. Iterme can be a list of elements, list of dataframes, list of arrays, or list of lists. List of lists up to 2 deep will be flattened to single list. A a few interesting use cases include: Passing None for the value of func acts as a shortcut to flatten nested lists. Using in place of map acts like a call to list(map(...)) Passing in pd.read_csv to list of files to get back a single dataframe Parameters: Name Type Description Default func callable function to apply. If None, will attempt to flatten a nested list required iterme iterable an iterable for which func will be called on each element required as_df bool; optional combine result into a DataFrame; Default False False as_arr bool; optional combine result into an array; Default False False axis int; optional what axis to concatenate over; Default 0 (first) 0 ignore_index bool; optional ignore the index when combining DataFrames; Default True True Source code in utilz/ops.py def apply ( func , iterme , as_df = False , as_arr = False , axis = 0 , ignore_index = True ): \"\"\" Applys func to iterme and combines the result into a single, list, dataFrame or array. Iterme can be a list of elements, list of dataframes, list of arrays, or list of lists. List of lists up to 2 deep will be flattened to single list. A a few interesting use cases include: - Passing None for the value of func acts as a shortcut to flatten nested lists. - Using in place of map acts like a call to list(map(...)) - Passing in pd.read_csv to list of files to get back a single dataframe Args: func (callable): function to apply. If None, will attempt to flatten a nested list iterme (iterable): an iterable for which func will be called on each element as_df (bool; optional): combine result into a DataFrame; Default False as_arr (bool; optional): combine result into an array; Default False axis (int; optional): what axis to concatenate over; Default 0 (first) ignore_index (bool; optional): ignore the index when combining DataFrames; Default True \"\"\" if as_df and as_arr : raise ValueError ( \"as_df and as_arr cannot both be True\" ) if func is None : return list ( chain . from_iterable ( iterme )) if isinstance ( iterme [ 0 ], list ): op = map ( func , chain . from_iterable ( iterme )) else : op = map ( func , iterme ) op = list ( op ) if as_df : op = pd . concat ( op , axis = axis , ignore_index = ignore_index ) elif as_arr : try : op = np . concatenate ( op , axis = axis ) except np . AxisError as e : # noqa warn ( \"Created new axis because requested concatenation axis > existing axes\" ) op = np . stack ( op , axis = axis - 1 ) return op norm_by_group ( df , grpcols , valcol , center = True , scale = True ) Normalize values in a column separately per group Parameters: Name Type Description Default df pd.DataFrame input dataframe required grpcols str/list grouping col(s) required valcol str value col required center bool mean center. Defaults to True. True scale bool divide by standard deviation. Defaults to True. True Source code in utilz/ops.py def norm_by_group ( df , grpcols , valcol , center = True , scale = True ): \"\"\" Normalize values in a column separately per group Args: df (pd.DataFrame): input dataframe grpcols (str/list): grouping col(s) valcol (str): value col center (bool, optional): mean center. Defaults to True. scale (bool, optional): divide by standard deviation. Defaults to True. \"\"\" def _norm ( dat , center , scale ): if center : dat = dat - dat . mean () if scale : dat = dat / dat . std () return dat return df . groupby ( grpcols )[ valcol ] . transform ( _norm , center , scale ) pmap ( func , iterme , func_args = None , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = 'processes' , progress = True , verbose = 0 , seed = None ) Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func Callable function to run required iterme Iterable an iterable for which each element will be passed to func required func_args list additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed Union[NoneType, int, numpy.random.mtrand.RandomState] random seed for reproducibility None Source code in utilz/ops.py def pmap ( func : Callable , iterme : Iterable , func_args : list = None , n_jobs : int = - 1 , loop_idx : bool = True , loop_random_seed : bool = False , backend : str = \"processes\" , progress : bool = True , verbose : int = 0 , seed : Union [ None , int , np . random . RandomState ] = None , ) -> Any : \"\"\" Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run iterme (iterable): an iterable for which each element will be passed to func func_args (list/dict/None): additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = len ( iterme )) if progress : iterator = tqdm ( len ( iterme )) else : iterator = len ( iterme ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func )( e , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args ) for e in iterme ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args ) for e in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out prep ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = 'processes' , progress = True , verbose = 0 , seed = None ) Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed int/None random seed for reproducibility None Examples: How to use a random seed. >>> from utilz.ops import ploop , random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum ( arr , seed = None ): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed ( seed ) >>> boot_arr = new_seed . choice ( arr , len ( arr ), replace = True ) >>> return boot_arr . sum () Finally call it in a parallel fashion >>> ploop ( boot_sum , [ np . arange ( 10 )], n_iter = 100 , loop_random_seed = True , loop_idx = False ) Source code in utilz/ops.py def prep ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = \"processes\" , progress = True , verbose = 0 , seed = None , ): \"\"\" Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility Examples: How to use a random seed. >>> from utilz.ops import ploop, random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum(arr, seed=None): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed(seed) >>> boot_arr = new_seed.choice(arr, len(arr), replace=True) >>> return boot_arr.sum() Finally call it in a parallel fashion >>> ploop(boot_sum, [np.arange(10)], n_iter=100, loop_random_seed=True, loop_idx=False) \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = n_iter ) if progress : iterator = tqdm ( range ( n_iter )) else : iterator = range ( n_iter ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func )( ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i }) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i }) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args ) for _ in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args ) for _ in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out random_seed ( seed ) Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state . Using the source here simply avoids an unecessary dependency. Parameters: Name Type Description Default seed None, int, np.RandomState iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. required Source code in utilz/ops.py def random_seed ( seed ): \"\"\"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to `sklearn.utils.check_random_state`. Using the source here simply avoids an unecessary dependency. Args: seed (None, int, np.RandomState): iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. \"\"\" import numbers if seed is None or seed is np . random : return np . random . mtrand . _rand if isinstance ( seed , ( numbers . Integral , np . integer )): return np . random . RandomState ( seed ) if isinstance ( seed , np . random . RandomState ): return seed raise ValueError ( \" %r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed ) splitdf ( df , X = None , Y = None ) Split a dataframe into X and Y arrays given column names. Useful for using a pandas dataframe for sklearn. If Y is None, assumes its in the first column. Parameters: Name Type Description Default df Dataframe input dataframe that's at least 2d required Y string name(s) of the columns for the Y array. Defaults to df.iloc[:, 0] None X string/list name(s) of columns for the X array. Defaults to df.iloc[:,1:] None Exceptions: Type Description ValueError If df is not 2 dimensional Returns: Type Description tuple (X, Y) Source code in utilz/ops.py def splitdf ( df , X = None , Y = None ): \"\"\" Split a dataframe into X and Y arrays given column names. Useful for using a pandas dataframe for sklearn. If Y is None, assumes its in the *first* column. Args: df (Dataframe): input dataframe that's at least 2d Y (string, optional): name(s) of the columns for the Y array. Defaults to df.iloc[:, 0] X (string/list, optional): name(s) of columns for the X array. Defaults to df.iloc[:,1:] Raises: ValueError: If df is not 2 dimensional Returns: tuple: (X, Y) \"\"\" if df . ndim != 2 : raise ValueError ( \"df must 2 dimensional\" ) if X is not None : x = df . loc [:, X ] . to_numpy ( copy = True ) else : x = df . iloc [:, 1 :] . to_numpy ( copy = True ) if Y is not None : y = df . loc [:, Y ] . to_numpy ( copy = True ) else : y = df . iloc [:, 0 ] . to_numpy ( copy = True ) return x , y","title":"Operations"},{"location":"api/ops/#utilzops","text":"","title":"utilz.ops"},{"location":"api/ops/#utilz.ops","text":"Common data operations and transformations often on pandas dataframes","title":"utilz.ops"},{"location":"api/ops/#utilz.ops.apply","text":"Applys func to iterme and combines the result into a single, list, dataFrame or array. Iterme can be a list of elements, list of dataframes, list of arrays, or list of lists. List of lists up to 2 deep will be flattened to single list. A a few interesting use cases include: Passing None for the value of func acts as a shortcut to flatten nested lists. Using in place of map acts like a call to list(map(...)) Passing in pd.read_csv to list of files to get back a single dataframe Parameters: Name Type Description Default func callable function to apply. If None, will attempt to flatten a nested list required iterme iterable an iterable for which func will be called on each element required as_df bool; optional combine result into a DataFrame; Default False False as_arr bool; optional combine result into an array; Default False False axis int; optional what axis to concatenate over; Default 0 (first) 0 ignore_index bool; optional ignore the index when combining DataFrames; Default True True Source code in utilz/ops.py def apply ( func , iterme , as_df = False , as_arr = False , axis = 0 , ignore_index = True ): \"\"\" Applys func to iterme and combines the result into a single, list, dataFrame or array. Iterme can be a list of elements, list of dataframes, list of arrays, or list of lists. List of lists up to 2 deep will be flattened to single list. A a few interesting use cases include: - Passing None for the value of func acts as a shortcut to flatten nested lists. - Using in place of map acts like a call to list(map(...)) - Passing in pd.read_csv to list of files to get back a single dataframe Args: func (callable): function to apply. If None, will attempt to flatten a nested list iterme (iterable): an iterable for which func will be called on each element as_df (bool; optional): combine result into a DataFrame; Default False as_arr (bool; optional): combine result into an array; Default False axis (int; optional): what axis to concatenate over; Default 0 (first) ignore_index (bool; optional): ignore the index when combining DataFrames; Default True \"\"\" if as_df and as_arr : raise ValueError ( \"as_df and as_arr cannot both be True\" ) if func is None : return list ( chain . from_iterable ( iterme )) if isinstance ( iterme [ 0 ], list ): op = map ( func , chain . from_iterable ( iterme )) else : op = map ( func , iterme ) op = list ( op ) if as_df : op = pd . concat ( op , axis = axis , ignore_index = ignore_index ) elif as_arr : try : op = np . concatenate ( op , axis = axis ) except np . AxisError as e : # noqa warn ( \"Created new axis because requested concatenation axis > existing axes\" ) op = np . stack ( op , axis = axis - 1 ) return op","title":"apply()"},{"location":"api/ops/#utilz.ops.norm_by_group","text":"Normalize values in a column separately per group Parameters: Name Type Description Default df pd.DataFrame input dataframe required grpcols str/list grouping col(s) required valcol str value col required center bool mean center. Defaults to True. True scale bool divide by standard deviation. Defaults to True. True Source code in utilz/ops.py def norm_by_group ( df , grpcols , valcol , center = True , scale = True ): \"\"\" Normalize values in a column separately per group Args: df (pd.DataFrame): input dataframe grpcols (str/list): grouping col(s) valcol (str): value col center (bool, optional): mean center. Defaults to True. scale (bool, optional): divide by standard deviation. Defaults to True. \"\"\" def _norm ( dat , center , scale ): if center : dat = dat - dat . mean () if scale : dat = dat / dat . std () return dat return df . groupby ( grpcols )[ valcol ] . transform ( _norm , center , scale )","title":"norm_by_group()"},{"location":"api/ops/#utilz.ops.pmap","text":"Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func Callable function to run required iterme Iterable an iterable for which each element will be passed to func required func_args list additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed Union[NoneType, int, numpy.random.mtrand.RandomState] random seed for reproducibility None Source code in utilz/ops.py def pmap ( func : Callable , iterme : Iterable , func_args : list = None , n_jobs : int = - 1 , loop_idx : bool = True , loop_random_seed : bool = False , backend : str = \"processes\" , progress : bool = True , verbose : int = 0 , seed : Union [ None , int , np . random . RandomState ] = None , ) -> Any : \"\"\" Map a function to iter me using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run iterme (iterable): an iterable for which each element will be passed to func func_args (list/dict/None): additional arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = len ( iterme )) if progress : iterator = tqdm ( len ( iterme )) else : iterator = len ( iterme ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func )( e , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : if loop_random_seed : out = parfor ( delayed ( func )( e , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"idx\" : i }) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args , ** { \"seed\" : seeds [ i ]}) for i , e in enumerate ( iterme ) ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( e , * func_args ) for e in iterme ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( e , ** func_args ) for e in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out","title":"pmap()"},{"location":"api/ops/#utilz.ops.prep","text":"Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Parameters: Name Type Description Default func callable function to run required func_args list/dict/None arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None None n_iter int number of iterations; Default 100 100 n_jobs int number of cpus/threads; Default -1 (all cpus/threads) -1 loop_idx bool whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True True loop_random_seed bool whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. False backend str 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' 'processes' progress bool whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. True verbose int joblib.Parallel verbosity. Default 0 0 seed int/None random seed for reproducibility None Examples: How to use a random seed. >>> from utilz.ops import ploop , random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum ( arr , seed = None ): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed ( seed ) >>> boot_arr = new_seed . choice ( arr , len ( arr ), replace = True ) >>> return boot_arr . sum () Finally call it in a parallel fashion >>> ploop ( boot_sum , [ np . arange ( 10 )], n_iter = 100 , loop_random_seed = True , loop_idx = False ) Source code in utilz/ops.py def prep ( func , func_args = None , n_iter = 100 , n_jobs =- 1 , loop_idx = True , loop_random_seed = False , backend = \"processes\" , progress = True , verbose = 0 , seed = None , ): \"\"\" Call a function for n_iter using parallelization via joblib. Note the only difference between pmap and prep is that that pmap explicitly operates on an iterable, such that the input to func changes each time (each element of iterme); where as prep just repeatedely executes func for n_iter operations with optional args/kwargs that are the same for each run of func. Args: func (callable): function to run func_args (list/dict/None): arguments to the function provided as a list for unnamed args or a dict for named kwargs. If None, assumes func takes no arguments excepted loop_idx_available (if its True); Default None n_iter (int, optional): number of iterations; Default 100 n_jobs (int, optional): number of cpus/threads; Default -1 (all cpus/threads) loop_idx (bool, optional): whether the value of the current iteration should be passed to func as the special kwarg 'idx'. Make sure func can handle a kwarg named 'idx'. Default True loop_random_seed (bool, optional): whether a randomly initialized seed should be passed to func as the special kwarg 'seed'. If func depends on any randomization (e.g. np.random) this should be set to True to ensure that parallel processes/threads use independent random seeds. Make sure func can handle a kwarg named 'seed' and utilize it for randomization. See example. Default False. backend (str, optional): 'processes' or 'threads'. Use 'threads' when you know you function releases Python's Global Interpreter Lock (GIL); Default 'cpus' progress (bool): whether to show a tqdm progress bar note, this may be a bit inaccurate when n_jobs > 1. Default True. verbose (int): joblib.Parallel verbosity. Default 0 seed (int/None): random seed for reproducibility Examples: How to use a random seed. >>> from utilz.ops import ploop, random_seed First make sure your function handles a 'seed' keyword argument. Then initialize it with the utilz.ops.random_seed function. Finally, use it internally where you would normally make a call to np.random. >>> def boot_sum(arr, seed=None): >>> \"Sum up elements of array after resampling with replacement\" >>> new_seed = random_seed(seed) >>> boot_arr = new_seed.choice(arr, len(arr), replace=True) >>> return boot_arr.sum() Finally call it in a parallel fashion >>> ploop(boot_sum, [np.arange(10)], n_iter=100, loop_random_seed=True, loop_idx=False) \"\"\" if backend not in [ \"processes\" , \"threads\" ]: raise ValueError ( \"backend must be one of cpu's threads\" ) parfor = Parallel ( prefer = backend , n_jobs = n_jobs , verbose = verbose ) if loop_random_seed : seeds = random_seed ( seed ) . randint ( MAX_INT , size = n_iter ) if progress : iterator = tqdm ( range ( n_iter )) else : iterator = range ( n_iter ) if func_args is None : if loop_idx : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func )( ** { \"idx\" : i }) for i in iterator ) else : if loop_random_seed : out = parfor ( delayed ( func )( ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : out = parfor ( delayed ( func ) for _ in iterator ) else : if loop_idx : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i , \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"idx\" : i }) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"idx\" : i }) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if loop_random_seed : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args , ** { \"seed\" : seeds [ i ]}) for i in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) else : if isinstance ( func_args , list ): out = parfor ( delayed ( func )( * func_args ) for _ in iterator ) elif isinstance ( func_args , dict ): out = parfor ( delayed ( func )( ** func_args ) for _ in iterator ) else : raise TypeError ( \"func_args must be a list or dict\" ) return out","title":"prep()"},{"location":"api/ops/#utilz.ops.random_seed","text":"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to sklearn.utils.check_random_state . Using the source here simply avoids an unecessary dependency. Parameters: Name Type Description Default seed None, int, np.RandomState iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. required Source code in utilz/ops.py def random_seed ( seed ): \"\"\"Turn seed into a np.random.RandomState instance. Note: credit for this code goes entirely to `sklearn.utils.check_random_state`. Using the source here simply avoids an unecessary dependency. Args: seed (None, int, np.RandomState): iff seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError. \"\"\" import numbers if seed is None or seed is np . random : return np . random . mtrand . _rand if isinstance ( seed , ( numbers . Integral , np . integer )): return np . random . RandomState ( seed ) if isinstance ( seed , np . random . RandomState ): return seed raise ValueError ( \" %r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed )","title":"random_seed()"},{"location":"api/ops/#utilz.ops.splitdf","text":"Split a dataframe into X and Y arrays given column names. Useful for using a pandas dataframe for sklearn. If Y is None, assumes its in the first column. Parameters: Name Type Description Default df Dataframe input dataframe that's at least 2d required Y string name(s) of the columns for the Y array. Defaults to df.iloc[:, 0] None X string/list name(s) of columns for the X array. Defaults to df.iloc[:,1:] None Exceptions: Type Description ValueError If df is not 2 dimensional Returns: Type Description tuple (X, Y) Source code in utilz/ops.py def splitdf ( df , X = None , Y = None ): \"\"\" Split a dataframe into X and Y arrays given column names. Useful for using a pandas dataframe for sklearn. If Y is None, assumes its in the *first* column. Args: df (Dataframe): input dataframe that's at least 2d Y (string, optional): name(s) of the columns for the Y array. Defaults to df.iloc[:, 0] X (string/list, optional): name(s) of columns for the X array. Defaults to df.iloc[:,1:] Raises: ValueError: If df is not 2 dimensional Returns: tuple: (X, Y) \"\"\" if df . ndim != 2 : raise ValueError ( \"df must 2 dimensional\" ) if X is not None : x = df . loc [:, X ] . to_numpy ( copy = True ) else : x = df . iloc [:, 1 :] . to_numpy ( copy = True ) if Y is not None : y = df . loc [:, Y ] . to_numpy ( copy = True ) else : y = df . iloc [:, 0 ] . to_numpy ( copy = True ) return x , y","title":"splitdf()"},{"location":"api/pipe/","text":"utilz.pipe utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok. Piping class similar to %>% in R. Pipe Pipe operator. Just initialize and assign to some variable to use immediately Example usage 1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe () # Some data to work with from seaborn import load_dataset df = load_dataset ( 'iris' ) 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Pipe Operator"},{"location":"api/pipe/#utilzpipe","text":"utilz proves a pipe operator similar to the %>% operator from magrittr in R. Note Doesn't currently work with numpy arrays. Other data types and pandas DataFrames are ok.","title":"utilz.pipe"},{"location":"api/pipe/#utilz.pipe","text":"Piping class similar to %>% in R.","title":"utilz.pipe"},{"location":"api/pipe/#utilz.pipe.Pipe","text":"Pipe operator. Just initialize and assign to some variable to use immediately","title":"Pipe"},{"location":"api/pipe/#example-usage","text":"1) First create the pipe object at the top of your code and name it whatever you want. I like 'o' from utilz.pipe import Pipe o = Pipe () # Some data to work with from seaborn import load_dataset df = load_dataset ( 'iris' ) 2) Then use it with the >>o>> syntax pipe to another function including lambdas (wrapped in parens) df >> o >> print df >> o >> ( lambda df : df * 2 ) pipe to a method and call it as a string (without the '.') df >>o>> 'head' pass args and kwargs to the method or function as a tuple. The first item in the tuple should be the method name (str) or function (callable). Subsequent items are interpreted as kwargs if they're dicts, or args if they're anything else. df >> o >> ( 'mean' , 1 ) # equivalent to df . mean ( 1 ) df >> o >> ( 'mean' , 1 , { 'numeric_only' : True } ) # df . mean ( 1 , numeric_only = True ) df >> o >> ( pd . melt , { 'id_vars' : 'species' } , { 'value_vars' : 'petal_length' } ) # pd . melt ( df , id_vars = 'species' , value_vars = 'petal_length' ) this is the same as above since melt is both a method on DataFrames and a module function in pandas df >>o>> ('melt', {'id_vars': 'species'}, {'value_vars': 'petal_length'}) # df.melt(id_vars='species', value_vars='petal_length') You can also combine kwargs into a single dict df >>o>> (pd.melt, {'id_vars': 'species', 'value_vars': 'petal_length'})","title":"Example usage"},{"location":"api/plot/","text":"utilz.plot Plotting convenience functions stripbarplot ( data , pointcolor = 'black' , xlabel = None , ylabel = None , xticklabels = None , yticklabels = None , xticks = None , yticks = None , xlim = None , ylim = None , * args , ** kwargs ) Combines a call to sns.barplot + sns.stripplot. Optionally set some axis level attributes during plot creation. Leaving these attributes None will return the default labels that seaborn sets. Source code in utilz/plot.py def stripbarplot ( data , pointcolor = \"black\" , xlabel = None , ylabel = None , xticklabels = None , yticklabels = None , xticks = None , yticks = None , xlim = None , ylim = None , * args , ** kwargs , ): \"\"\" Combines a call to sns.barplot + sns.stripplot. Optionally set some axis level attributes during plot creation. Leaving these attributes None will return the default labels that seaborn sets. \"\"\" ax = sns . barplot ( * args , ** kwargs , data = data ) ax = sns . stripplot ( * args , ** kwargs , color = pointcolor , data = data , ax = ax ) if xlabel : ax . set_xlabel ( xlabel ) if ylabel : ax . set_ylabel ( ylabel ) if xticklabels : ax . set_xticklabels ( xticklabels ) if yticklabels : ax . set_yticklabels ( yticklabels ) if xticks : ax . set_xticks ( xticks ) if yticks : ax . set_yticks ( yticks ) if xlim : ax . set ( xlim = xlim ) if ylim : ax . set ( ylim = ylim ) return ax","title":"Plotting Helpers"},{"location":"api/plot/#utilzplot","text":"","title":"utilz.plot"},{"location":"api/plot/#utilz.plot","text":"Plotting convenience functions","title":"utilz.plot"},{"location":"api/plot/#utilz.plot.stripbarplot","text":"Combines a call to sns.barplot + sns.stripplot. Optionally set some axis level attributes during plot creation. Leaving these attributes None will return the default labels that seaborn sets. Source code in utilz/plot.py def stripbarplot ( data , pointcolor = \"black\" , xlabel = None , ylabel = None , xticklabels = None , yticklabels = None , xticks = None , yticks = None , xlim = None , ylim = None , * args , ** kwargs , ): \"\"\" Combines a call to sns.barplot + sns.stripplot. Optionally set some axis level attributes during plot creation. Leaving these attributes None will return the default labels that seaborn sets. \"\"\" ax = sns . barplot ( * args , ** kwargs , data = data ) ax = sns . stripplot ( * args , ** kwargs , color = pointcolor , data = data , ax = ax ) if xlabel : ax . set_xlabel ( xlabel ) if ylabel : ax . set_ylabel ( ylabel ) if xticklabels : ax . set_xticklabels ( xticklabels ) if yticklabels : ax . set_yticklabels ( yticklabels ) if xticks : ax . set_xticks ( xticks ) if yticks : ax . set_yticks ( yticks ) if xlim : ax . set ( xlim = xlim ) if ylim : ax . set ( ylim = ylim ) return ax","title":"stripbarplot()"},{"location":"api/termplot/","text":"utilz.termplot Plotting module dedicated to working with plots in an interactive terminal (not jupyter notebook!) init_termplot () Initilize terminal based plotting. Import and run this before any other python plotting module! e.g. before matplotlib. Requires the imgcat command line program available in Iterm. Source code in utilz/termplot.py def init_termplot (): \"\"\" Initilize terminal based plotting. **Import and run this before any other python plotting module!** e.g. before matplotlib. Requires the `imgcat` command line program available in Iterm. \"\"\" import matplotlib matplotlib . use ( \"module://imgcat\" ) p ( obj , * args , ** kwargs ) Show a plot in the terminal using an object's own .plot method. No need to call utilz.termplot.s() if using this function. Parameters: Name Type Description Default obj Any a python object that has a .plot() method required args Any arguments to the object's .plot() method () kwargs Any keyword arguments to the object's .plot() method {} Examples: >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}) >>> p ( df ) >>> f , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 3 )) >>> out = plot ( ax , [ 1 , 2 , 3 ]) Source code in utilz/termplot.py def p ( obj , * args , ** kwargs ): \"\"\" Show a plot in the terminal using an object's own .plot method. No need to call `utilz.termplot.s()` if using this function. Args: obj (Any): a python object that has a `.plot()` method args (Any): arguments to the object's `.plot()` method kwargs (Any): keyword arguments to the object's `.plot()` method Examples: >>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}) >>> p(df) >>> f, ax = plt.subplots(1, 1, figsize=(4, 3)) >>> out = plot(ax, [1, 2, 3]) \"\"\" plot = getattr ( obj , \"plot\" , None ) if callable ( plot ): out = obj . plot ( * args , ** kwargs ) s () return out else : raise TypeError ( f \"Object of type { type ( obj ) } has not .plot() method!\" ) s () Show a plot in the terminal and immediately close the figure handle to save memory. This has to be called immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using utilz.termplot.p() , which will automatically call this function Examples: >>> plt . plot ([ 1 , 2 , 3 ]) >>> s () >>> sns . scatterplot ( 'x' , 'y' , data = df ) >>> s () Source code in utilz/termplot.py def s (): \"\"\" Show a plot in the terminal and immediately close the figure handle to save memory. This **has to be called** immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using `utilz.termplot.p()`, which will automatically call this function Examples: >>> plt.plot([1,2,3]) >>> s() >>> sns.scatterplot('x','y',data=df) >>> s() \"\"\" if \"plt\" not in dir (): import matplotlib.pyplot as plt if len ( plt . get_fignums ()): plt . show () plt . close () else : raise ValueError ( \"No matplotlib figures found. Are you sure you plotted something?\" )","title":"Terminal Plots"},{"location":"api/termplot/#utilztermplot","text":"","title":"utilz.termplot"},{"location":"api/termplot/#utilz.termplot","text":"Plotting module dedicated to working with plots in an interactive terminal (not jupyter notebook!)","title":"utilz.termplot"},{"location":"api/termplot/#utilz.termplot.init_termplot","text":"Initilize terminal based plotting. Import and run this before any other python plotting module! e.g. before matplotlib. Requires the imgcat command line program available in Iterm. Source code in utilz/termplot.py def init_termplot (): \"\"\" Initilize terminal based plotting. **Import and run this before any other python plotting module!** e.g. before matplotlib. Requires the `imgcat` command line program available in Iterm. \"\"\" import matplotlib matplotlib . use ( \"module://imgcat\" )","title":"init_termplot()"},{"location":"api/termplot/#utilz.termplot.p","text":"Show a plot in the terminal using an object's own .plot method. No need to call utilz.termplot.s() if using this function. Parameters: Name Type Description Default obj Any a python object that has a .plot() method required args Any arguments to the object's .plot() method () kwargs Any keyword arguments to the object's .plot() method {} Examples: >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}) >>> p ( df ) >>> f , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 3 )) >>> out = plot ( ax , [ 1 , 2 , 3 ]) Source code in utilz/termplot.py def p ( obj , * args , ** kwargs ): \"\"\" Show a plot in the terminal using an object's own .plot method. No need to call `utilz.termplot.s()` if using this function. Args: obj (Any): a python object that has a `.plot()` method args (Any): arguments to the object's `.plot()` method kwargs (Any): keyword arguments to the object's `.plot()` method Examples: >>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}) >>> p(df) >>> f, ax = plt.subplots(1, 1, figsize=(4, 3)) >>> out = plot(ax, [1, 2, 3]) \"\"\" plot = getattr ( obj , \"plot\" , None ) if callable ( plot ): out = obj . plot ( * args , ** kwargs ) s () return out else : raise TypeError ( f \"Object of type { type ( obj ) } has not .plot() method!\" )","title":"p()"},{"location":"api/termplot/#utilz.termplot.s","text":"Show a plot in the terminal and immediately close the figure handle to save memory. This has to be called immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using utilz.termplot.p() , which will automatically call this function Examples: >>> plt . plot ([ 1 , 2 , 3 ]) >>> s () >>> sns . scatterplot ( 'x' , 'y' , data = df ) >>> s () Source code in utilz/termplot.py def s (): \"\"\" Show a plot in the terminal and immediately close the figure handle to save memory. This **has to be called** immediately after any normal python plotting function in order to render a plot in the terminal. The exception is if you're using `utilz.termplot.p()`, which will automatically call this function Examples: >>> plt.plot([1,2,3]) >>> s() >>> sns.scatterplot('x','y',data=df) >>> s() \"\"\" if \"plt\" not in dir (): import matplotlib.pyplot as plt if len ( plt . get_fignums ()): plt . show () plt . close () else : raise ValueError ( \"No matplotlib figures found. Are you sure you plotted something?\" )","title":"s()"},{"location":"api/verbs/","text":"utilz.verbs dplyr like verbs for working with pandas dataframes. Designed to be piped together using the pipe function from the toolz package. Provides a broad use-case alternative to plydata or larger packages while use mostly native pandas methods under-the-hood. Note : the current version unfortunately does borrow the select, and define functions from plydata Workhorses rows : subset rows based on some conditions (str), ranges (tuples), or indices (list) cols : subset cols based on some conditions (str), ranges (tuples), or indices (list); support \"-col\" inversion -> from plydata summarize : create a new column(s) thats the result of a operation that returns a scalar value and assigns it back to df. Accepts dfs or grouped dfs assign : create a new column(s) thats the result of a operation that returns a series value and assigns it back to df. Accepts dfs or grouped dfs I think summarize -> aggregate and assign -> mutate in dplyr land Secondary apply : to apply artbitrary functions on a df or grouped df (just a wrapper around df.apply) Example Verb-based analysis of a dataframe from toolz import pipe from utilz import rows , cols # Basic slicing/subsetting pipe ( x , rows ( \"group == 'c' or group == 'b'\" ), cols ( \"rt\" , \"speed\" ) ) pipe ( x , rows ([ 1 , 9 , 14 ]), cols (( 3 , 5 )) ) pipe ( x , rows (( 1 , 11 , 2 )) ) # Perform scalar operation that always return back a *smaller* dataframe or series than the original # Non-grouped inputs produce series results, while grouped inputs produce dataframe results pipe ( x , rows ( \"group == 'c' or group == 'b'\" ), summarize ( rt = 'mean' , speed = 'mean' ) ) pipe ( x , groupby ( 'group' ), summarize ( score = 'mean' , rt = 'std' ) ) # Perform series operations that always return back a dataframe thats the *same* size as the original # To preserve input size, use assign instead. It will \"broadcast\" a smaller set of values over the length of the entire input dataframe, while respecting groups if the input is grouped pipe ( x , assign ( score_centered = 'score.mean()' )) # In this case the return values are computed and broadcasted within group rather than across the entire frame pipe ( x , groupby ( 'group' ), assign ( score_centered = 'score - score.mean()' , score_norm = 'score/score.std()' ) ) # e.g. the mean in sub-group pipe ( x , groupby ( 'group' ), assign ( speed_per_group = 'speed.mean()' ) )","title":"Data analysis \"verbs\""},{"location":"api/verbs/#utilzverbs","text":"","title":"utilz.verbs"},{"location":"api/verbs/#utilz.verbs","text":"dplyr like verbs for working with pandas dataframes. Designed to be piped together using the pipe function from the toolz package. Provides a broad use-case alternative to plydata or larger packages while use mostly native pandas methods under-the-hood. Note : the current version unfortunately does borrow the select, and define functions from plydata","title":"utilz.verbs"},{"location":"api/verbs/#workhorses","text":"rows : subset rows based on some conditions (str), ranges (tuples), or indices (list) cols : subset cols based on some conditions (str), ranges (tuples), or indices (list); support \"-col\" inversion -> from plydata summarize : create a new column(s) thats the result of a operation that returns a scalar value and assigns it back to df. Accepts dfs or grouped dfs assign : create a new column(s) thats the result of a operation that returns a series value and assigns it back to df. Accepts dfs or grouped dfs I think summarize -> aggregate and assign -> mutate in dplyr land","title":"Workhorses"},{"location":"api/verbs/#secondary","text":"apply : to apply artbitrary functions on a df or grouped df (just a wrapper around df.apply)","title":"Secondary"},{"location":"api/verbs/#example-verb-based-analysis-of-a-dataframe","text":"from toolz import pipe from utilz import rows , cols # Basic slicing/subsetting pipe ( x , rows ( \"group == 'c' or group == 'b'\" ), cols ( \"rt\" , \"speed\" ) ) pipe ( x , rows ([ 1 , 9 , 14 ]), cols (( 3 , 5 )) ) pipe ( x , rows (( 1 , 11 , 2 )) ) # Perform scalar operation that always return back a *smaller* dataframe or series than the original # Non-grouped inputs produce series results, while grouped inputs produce dataframe results pipe ( x , rows ( \"group == 'c' or group == 'b'\" ), summarize ( rt = 'mean' , speed = 'mean' ) ) pipe ( x , groupby ( 'group' ), summarize ( score = 'mean' , rt = 'std' ) ) # Perform series operations that always return back a dataframe thats the *same* size as the original # To preserve input size, use assign instead. It will \"broadcast\" a smaller set of values over the length of the entire input dataframe, while respecting groups if the input is grouped pipe ( x , assign ( score_centered = 'score.mean()' )) # In this case the return values are computed and broadcasted within group rather than across the entire frame pipe ( x , groupby ( 'group' ), assign ( score_centered = 'score - score.mean()' , score_norm = 'score/score.std()' ) ) # e.g. the mean in sub-group pipe ( x , groupby ( 'group' ), assign ( speed_per_group = 'speed.mean()' ) )","title":"Example Verb-based analysis of a dataframe"}]}